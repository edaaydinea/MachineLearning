{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome\n",
    "\n",
    "#### Course Structure\n",
    "- **First Two Courses:** Focused on supervised learning.\n",
    "- **This Course:** Introduces unsupervised learning, recommender systems, and reinforcement learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Week 1: **Clustering Algorithms & Anomaly Detection**\n",
    "- **Clustering:** Groups data into clusters based on similarity.\n",
    "  - Widely used for pattern discovery in data.\n",
    "- **Anomaly Detection:** Identifies outliers in data.\n",
    "  - Critical for detecting unusual behavior in commercial applications (e.g., fraud detection).\n",
    "\n",
    "By the end of the week, you'll:\n",
    "- Understand how clustering and anomaly detection algorithms work.\n",
    "- Be able to implement and apply them.\n",
    "\n",
    "---\n",
    "\n",
    "### Week 2: **Recommender Systems**\n",
    "- **Importance:** Key technology in e-commerce and online services.\n",
    "  - Drives billions in value by recommending products, movies, or ads.\n",
    "- **Underrepresented in Academia:** Despite its importance, recommender systems receive little academic attention.\n",
    "- **Applications:** Online shopping, video streaming, and ad recommendation systems.\n",
    "\n",
    "By the end of the week, you'll:\n",
    "- Learn how recommender systems operate.\n",
    "- Implement one yourself.\n",
    "- Understand how online ad tech decides what ads to display.\n",
    "\n",
    "---\n",
    "\n",
    "### Week 3: **Reinforcement Learning (RL)**\n",
    "- **Overview:** RL excels at tasks like playing video games and controlling robots.\n",
    "- **Emerging Technology:** Not as many commercial applications yet but growing rapidly.\n",
    "- **Applications:** Video games, robotics, and autonomous systems.\n",
    "\n",
    "By the end of the week, you'll:\n",
    "- Implement RL to land a simulated moon lander.\n",
    "- Experience firsthand the potential of RL for complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "- **Unsupervised Learning:** Clustering, anomaly detection.\n",
    "- **Recommender Systems:** Drives commercial value, practical implementation.\n",
    "- **Reinforcement Learning:** Exciting, emerging technology with impressive potential.\n",
    "\n",
    "This course will add powerful techniques beyond supervised learning to your machine learning toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Clustering?\n",
    "\n",
    "#### Definition of Clustering:\n",
    "- **Clustering** is an unsupervised learning algorithm that looks at data points and groups them based on their similarity.\n",
    "- Unlike **supervised learning** (which uses labeled data), clustering only uses input features **x** and does not have target labels **y** to predict.\n",
    "- The algorithm identifies patterns or structures within the data without predefined categories.\n",
    "\n",
    "#### Contrast with Supervised Learning:\n",
    "- **Supervised Learning:** Uses both input features **x** and target labels **y** to fit a model (e.g., logistic regression or neural network).\n",
    "  - The model predicts labels based on input data.\n",
    "  - Example: Learning a decision boundary for binary classification.\n",
    "- **Unsupervised Learning (Clustering):** Only has **x** values, so the task is to find interesting patterns in the data.\n",
    "  - The goal is to group data points into clusters based on their similarity.\n",
    "\n",
    "#### Example:\n",
    "- A dataset with features **x_1** and **x_2** (no labels) might be grouped into two clusters using a clustering algorithm, identifying similar data points.\n",
    "  \n",
    "#### Applications of Clustering:\n",
    "- **News Article Grouping:** Grouping similar news articles into clusters based on content (e.g., stories about pandas).\n",
    "- **Market Segmentation:** Discovering clusters of users with similar goals (e.g., skill development or career growth).\n",
    "- **DNA Data Analysis:** Grouping individuals with similar genetic expression to identify patterns in traits.\n",
    "- **Astronomy:** Clustering astronomical data to group celestial bodies and understand coherent structures like galaxies.\n",
    "\n",
    "#### Next Topic:\n",
    "- **k-Means Algorithm:** The most commonly used clustering algorithm will be discussed next.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "- **Clustering**: Groups similar data points in unsupervised learning.\n",
    "- **No Labels**: Works without target labels, identifying patterns and structures in the dataset.\n",
    "- **Applications**: Widely used in news, market segmentation, DNA analysis, and astronomy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means intuition\n",
    "\n",
    "#### Overview of K-means:\n",
    "- **K-means** is a commonly used clustering algorithm that groups data points into **k clusters**.\n",
    "- The algorithm iteratively assigns points to cluster centroids and updates the centroids based on the assigned points.\n",
    "\n",
    "#### Steps of the K-means Algorithm:\n",
    "1. **Random Initialization:**\n",
    "   - K-means begins by **randomly guessing** the initial positions of the cluster centroids.\n",
    "   - The number of centroids is determined by **k**, the number of clusters (e.g., 2 clusters in this example).\n",
    "\n",
    "2. **Assignment Step:**\n",
    "   - For each data point, K-means checks whether it is closer to one centroid or another.\n",
    "   - Points are assigned to the nearest centroid.\n",
    "   - This step effectively **groups data points** into clusters based on proximity to the centroids.\n",
    "   - The assigned points are represented by different colors for each cluster (e.g., red and blue in this example).\n",
    "\n",
    "3. **Centroid Update Step:**\n",
    "   - After assigning points to centroids, the algorithm computes the **average location (mean)** of all points in each cluster.\n",
    "   - The centroid is moved to the new average location of its assigned points.\n",
    "   \n",
    "4. **Repeat Steps:**\n",
    "   - The assignment and centroid update steps are **repeated iteratively**:\n",
    "     - Points are reassigned to the nearest centroid.\n",
    "     - Centroids are moved based on the new assignments.\n",
    "   - As the centroids move, some points may switch clusters (change color).\n",
    "\n",
    "5. **Convergence:**\n",
    "   - K-means continues until no further changes occur:\n",
    "     - The assignments of points to centroids stop changing.\n",
    "     - The centroids no longer move.\n",
    "   - At this point, the algorithm is said to have **converged**.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Cluster Centroids:** The central point of a cluster, updated as the mean of the points in that cluster.\n",
    "- **Assignment and Update:** K-means alternates between assigning points to the nearest centroid and updating centroids based on assigned points.\n",
    "- **Convergence:** The algorithm stops when further assignments and centroid updates no longer result in changes.\n",
    "\n",
    "#### Summary of Steps:\n",
    "1. Initialize centroids randomly.\n",
    "2. Assign each point to the nearest centroid.\n",
    "3. Move centroids to the mean of assigned points.\n",
    "4. Repeat until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "- **Iterative Process:** K-means repeatedly assigns points and updates centroids.\n",
    "- **Convergence:** The algorithm stops when no more changes occur.\n",
    "- **Centroid Updates:** Centroids are adjusted to the average of their assigned points.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means algorithm\n",
    "\n",
    "The K-means algorithm is a popular clustering technique that aims to group unlabeled data into $K$ clusters by repeatedly performing two main steps: assigning data points to cluster centroids and updating the cluster centroids' positions based on the points assigned to them. Here's a detailed breakdown of the algorithm:\n",
    "\n",
    "### Steps of the K-means Algorithm:\n",
    "1. **Random Initialization:**\n",
    "   - Randomly initialize $K$ cluster centroids, denoted as $\\mu_1, \\mu_2, \\dots, \\mu_K$.\n",
    "   - These centroids are vectors with the same dimension as the data points. For example, if each data point has two features (as in a 2D plane), each centroid is a 2D vector.\n",
    "\n",
    "2. **Assign Points to Closest Cluster Centroid (Step 1):**\n",
    "   - For each training example $x^i$ (where $i = 1, \\dots, m$), compute the distance from $x^i$ to each of the $K$ centroids and assign the point to the closest centroid.\n",
    "   - The assignment step can be expressed as:\n",
    "     $$c^i = \\arg \\min_k \\|x^i - \\mu_k\\|^2$$\n",
    "     Where $c^i$ is the index of the cluster centroid closest to $x^i$, and $\\|x^i - \\mu_k\\|$ represents the Euclidean distance between the point and centroid.\n",
    "\n",
    "3. **Move Centroids (Step 2):**\n",
    "   - Once all the points are assigned to centroids, recompute the centroid positions by calculating the average (mean) of all points assigned to each centroid.\n",
    "   - The centroid update formula for the $k$-th cluster is:\n",
    "     $$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^i$$\n",
    "     Where $C_k$ is the set of points assigned to cluster $k$ and $|C_k|$ is the number of points in that cluster.\n",
    "\n",
    "4. **Repeat Until Convergence:**\n",
    "   - Repeat steps 2 and 3 (assign points to centroids and move centroids) until the algorithm converges, meaning the centroids no longer change or the point assignments remain the same.\n",
    "\n",
    "### Corner Case:\n",
    "- If a cluster has no points assigned to it, the algorithm cannot compute the mean for that cluster. In such cases:\n",
    "  - One option is to eliminate the empty cluster, reducing the number of clusters.\n",
    "  - Another option is to randomly reinitialize the centroid and continue the algorithm.\n",
    "\n",
    "### Application:\n",
    "- K-means is commonly applied to data with distinct clusters but can also be useful when clusters are less well-defined. For instance, in determining t-shirt sizes (small, medium, large), K-means can help cluster customers' heights and weights, even if there are no clear boundaries between size categories.\n",
    "\n",
    "### Key Insights:\n",
    "- The algorithm alternates between assigning points to the nearest centroids and updating centroid positions.\n",
    "- K-means aims to minimize the total within-cluster variance (sum of squared distances between points and their corresponding centroids).\n",
    "- The algorithm is guaranteed to converge, though it may converge to local minima depending on the initial centroid positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization objective\n",
    "\n",
    "This part of your study on **K-means clustering** introduces the cost function it minimizes. Here's a summary:\n",
    "\n",
    "1. **Cost Function (Distortion Function)**: \n",
    "   The K-means algorithm minimizes the squared distance between each data point and its assigned cluster centroid. The cost function J is defined as the average of these squared distances across all data points.\n",
    "\n",
    "   - **Notation**:\n",
    "     - $C_i$: Index of the cluster to which training example $X_i$ is assigned.\n",
    "     - $\\mu_k$: Location of the $k$-th cluster centroid.\n",
    "     - The cost function is:  \n",
    "       $$J = \\frac{1}{m} \\sum_{i=1}^{m} ||X_i - \\mu_{C_i}||^2$$\n",
    "       where $||X_i - \\mu_{C_i}||^2$ is the squared distance between $X_i$ and its assigned cluster centroid.\n",
    "\n",
    "2. **Two-Step Process**:\n",
    "   - **Step 1: Assigning points to centroids**:\n",
    "     Each point $X_i$ is assigned to the cluster centroid $\\mu_k$ closest to it (minimizing the squared distance for fixed centroids).\n",
    "   - **Step 2: Updating centroids**:\n",
    "     The algorithm recalculates the cluster centroids by taking the average of all points assigned to each cluster, which minimizes the cost function for fixed assignments.\n",
    "\n",
    "3. **Convergence**:\n",
    "   - Each iteration of K-means either reduces or maintains the cost function. If the cost function stops changing, the algorithm has converged.\n",
    "   - If the cost decreases very slowly, you may stop the algorithm, similar to the behavior of gradient descent when it nears a minimum.\n",
    "\n",
    "4. **Multiple Initializations**:\n",
    "   - By using different random initializations for cluster centroids, K-means can often find better clustering results. This is particularly useful because K-means is sensitive to initial centroid positions.\n",
    "\n",
    "The process shows how the cost function helps determine convergence and improve the quality of clusters with multiple random starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing K-means\n",
    "\n",
    "In this lecture, the focus is on the first step of the K-means clustering algorithm: selecting initial cluster centroids. Here's a summary of key points:\n",
    "\n",
    "1. **Random Initialization**: \n",
    "   - The most common method for choosing initial centroids is to randomly select **K training examples** from the dataset. These selected examples become the initial cluster centroids. For instance, if K=2, you might randomly pick two points from the dataset and assign them as the red and blue centroids.\n",
    "   \n",
    "2. **Effect of Initialization**:\n",
    "   - Different random initializations can lead to different clustering results. Some may lead to optimal clusters, while others may get stuck in local minima (suboptimal solutions).\n",
    "   - For example, in a 3-cluster problem, a \"good\" initialization may give clear and distinct clusters, but a \"bad\" initialization could result in poorly separated clusters.\n",
    "\n",
    "3. **Multiple Attempts**: \n",
    "   - To increase the likelihood of finding a good clustering, it's recommended to run K-means multiple times with different random initializations. \n",
    "   - After each run, you calculate the **distortion cost function (J)**, which measures the sum of squared distances between points and their corresponding centroids.\n",
    "   - By comparing the cost function values, you can select the clustering with the **lowest cost**, which represents the best set of clusters.\n",
    "\n",
    "4. **Number of Initializations**: \n",
    "   - Running K-means multiple times (e.g., between 50 to 1000 attempts) is a common practice to ensure better results. Running it excessively (e.g., more than 1000 times) tends to offer diminishing returns and becomes computationally expensive.\n",
    "   \n",
    "5. **Conclusion**:\n",
    "   - The main takeaway is to use more than one random initialization in K-means to minimize the chances of getting stuck in a local minimum. This ensures better optimization of the distortion cost function and a higher likelihood of finding the best clustering solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of clusters\n",
    "\n",
    "In this lecture, the key focus is on how to choose the number of clusters, **K**, in the K-means clustering algorithm. The instructor highlights that determining the right number of clusters can be ambiguous, as clustering is an unsupervised learning task. Different people may perceive different numbers of clusters in the same dataset, and both answers could be valid. This is because clustering doesn't come with predefined labels to guide the selection of **K**.\n",
    "\n",
    "Here are the main points covered:\n",
    "\n",
    "1. **Ambiguity of K**: \n",
    "   - Clustering does not have a definitive \"correct\" number of clusters. For instance, a dataset could be interpreted as having two or four clusters, and both interpretations might be valid.\n",
    "\n",
    "2. **The Elbow Method**:\n",
    "   - One way to choose **K** is to plot the cost function **J** (or distortion function) as a function of **K**. \n",
    "   - As **K** increases, the cost function decreases, and the \"elbow\" in the plot may suggest the right value of **K**. \n",
    "   - The idea is to find a point where increasing **K** doesn't significantly reduce the cost function. However, the instructor mentions that they rarely use this method in practice, as the cost function often decreases smoothly without a clear elbow.\n",
    "\n",
    "3. **Minimizing J**:\n",
    "   - Simply choosing **K** to minimize the cost function **J** will lead to the selection of the largest possible value of **K**. This approach is not recommended because more clusters will always reduce the cost function, but it won't necessarily provide a meaningful clustering structure.\n",
    "\n",
    "4. **Choosing K Based on Downstream Purpose**:\n",
    "   - Often, K-means clustering is used for a specific downstream purpose, and the number of clusters should be chosen based on how well the clusters serve that purpose. \n",
    "   - For example, in the case of t-shirt sizing, using three clusters might result in small, medium, and large sizes, while five clusters might result in extra small, small, medium, large, and extra-large. The choice between three or five clusters depends on the trade-off between better fit (with more sizes) and the cost of manufacturing and handling more t-shirt sizes.\n",
    "\n",
    "5. **Trade-off in Other Applications**:\n",
    "   - Another example provided is image compression. The choice of **K** affects the trade-off between image quality and compression size. A higher **K** results in better image quality but less compression, whereas a lower **K** results in more compression but lower image quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding unusual events\n",
    "\n",
    "**Overview:**\n",
    "- **Anomaly detection** identifies unusual or anomalous events by learning from a dataset of normal events.\n",
    "- Commonly used when abnormal or defective examples are rare (e.g., aircraft engines, fraud detection).\n",
    "- **Applications**: Manufacturing quality control, fraud detection, monitoring computer clusters, and telco networks.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Features (x1, x2)**: Represent different aspects (e.g., heat, vibrations for engines).\n",
    "- **Goal**: To detect whether new data points (e.g., new engine's features) are consistent with past normal examples.\n",
    "\n",
    "**Steps in Anomaly Detection:**\n",
    "1. **Collect Training Data**: Build a dataset of normal examples (e.g., well-functioning engines) with multiple features.\n",
    "2. **Density Estimation**:\n",
    "   - **Estimate Probability Distribution (p(x))**: Determine regions of high and low probability for feature values.\n",
    "   - Example: High-probability regions for features lie in central ellipses; lower probabilities are further out.\n",
    "3. **Compute Probability (p(x_test))**:\n",
    "   - New data points are evaluated to check if they fit the normal pattern.\n",
    "   - **Threshold (ε)**: If p(x_test) < ε, it is flagged as an anomaly.\n",
    "\n",
    "**Practical Examples**:\n",
    "- **Aircraft Engines**: If a new engine’s heat/vibration pattern deviates significantly from past data, it’s marked for inspection.\n",
    "- **Fraud Detection**: Monitor user behavior (e.g., login frequency, typing speed) for anomalies in activities.\n",
    "- **Manufacturing**: Identify defective products by detecting abnormal behavior in components like circuit boards or smartphones.\n",
    "- **Computer Clusters**: Identify system failures (e.g., CPU, memory, network issues) by monitoring deviations in system metrics.\n",
    "\n",
    "**Algorithm Workflow**:\n",
    "1. Learn p(x) from normal training data.\n",
    "2. For each new data point, compute p(x_test).\n",
    "3. Flag as an anomaly if p(x_test) < ε.\n",
    "\n",
    "**Use Cases**:\n",
    "- **Fraud detection**: Identifying fake accounts or unusual financial transactions.\n",
    "- **Manufacturing**: Detecting defects in aircraft engines, smartphones, printed circuit boards, etc.\n",
    "- **IT Systems**: Monitoring computer systems for unusual behavior indicating hardware or network failures.\n",
    "\n",
    "**Important Points**:\n",
    "- **Gaussian distribution**: Commonly used to model the data p(x) in anomaly detection.\n",
    "- Anomaly detection is widely used despite not always being discussed frequently in industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian (normal) distribution\n",
    "\n",
    "This explanation focuses on the **Gaussian distribution (normal distribution)** and its role in anomaly detection. Here's a breakdown:\n",
    "\n",
    "### **Gaussian (Normal) Distribution:**\n",
    "- The **Gaussian distribution** is also called the **bell-shaped curve** because of its shape.\n",
    "- It is defined by two parameters:\n",
    "  1. **Mean (μ)**: Determines the center of the curve.\n",
    "  2. **Standard Deviation (σ)**: Determines the spread or width of the curve. The square of σ is called the **variance (σ²)**.\n",
    "\n",
    "### **Gaussian Probability Function (p(x)):**\n",
    "- The probability of a random variable **x** can be described by a formula:\n",
    "  $$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
    "  - **μ** is the mean.\n",
    "  - **σ²** is the variance.\n",
    "  - **π** is the constant (approximately 3.14159).\n",
    "  - **e** is Euler's number.\n",
    "\n",
    "### **Impact of Mean (μ) and Standard Deviation (σ):**\n",
    "1. **μ controls the center**: Changing the mean moves the curve left or right.\n",
    "2. **σ controls the width**: Smaller σ makes the curve narrower, while larger σ makes it wider.\n",
    "\n",
    "### **Example Behavior:**\n",
    "- **σ = 1**: Standard width.\n",
    "- **σ = 0.5**: Narrower and taller curve.\n",
    "- **σ = 2**: Wider and shorter curve.\n",
    "- **Shifting μ**: Moves the peak of the distribution to a different value of x.\n",
    "\n",
    "### **Histogram Interpretation:**\n",
    "- A histogram of a dataset that follows a normal distribution will approximate the bell-shaped curve, especially with a large sample size.\n",
    "- The Gaussian curve represents the probability distribution for infinite samples.\n",
    "\n",
    "### **Anomaly Detection Using Gaussian Distribution:**\n",
    "- The idea is to model the **normal data** using a Gaussian distribution.\n",
    "- For each new data point:\n",
    "  - Calculate **p(x)** using the Gaussian formula.\n",
    "  - If **p(x)** is lower than a certain threshold (ε), the point is flagged as an anomaly.\n",
    "- **Multivariate Gaussian**: Extends this method to multiple features (variables), where instead of one feature (x), you work with several.\n",
    "\n",
    "### **Parameter Estimation:**\n",
    "- **Mean (μ)**: The average of the training data.\n",
    "  $$\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}$$\n",
    "- **Variance (σ²)**: The average of the squared differences between the data points and the mean.\n",
    "  $$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\mu)^2$$\n",
    "- These are called the **maximum likelihood estimates** of μ and σ².\n",
    "\n",
    "### **Next Steps:**\n",
    "- Anomaly detection becomes more complex with multiple features. The basic principles for one feature extend to cases where you have many features, allowing you to detect anomalies in higher-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection algorithm\n",
    "\n",
    "This section explains how to build an anomaly detection system using Gaussian or normal distributions for each feature in a dataset. Here's a breakdown of the process:\n",
    "\n",
    "### 1. **Training Set and Feature Selection**:\n",
    "   - You are given a training set with examples $x_1, x_2, \\dots, x_m$, where each example $x$ has $n$ features (e.g., heat and vibration in an airplane engine).\n",
    "   - The goal is to estimate the probability $p(x)$ of any given feature vector $x$, which is done by assuming the features are statistically independent.\n",
    "\n",
    "### 2. **Modeling Feature Probabilities**:\n",
    "   - The probability $p(x)$ for a feature vector $x$ is modeled as:\n",
    "     $$p(x) = p(x_1) \\times p(x_2) \\times \\dots \\times p(x_n)$$\n",
    "     Each feature $x_j$ is modeled with a Gaussian distribution using parameters $\\mu_j$ (mean) and $\\sigma_j^2$ (variance). For each feature, you compute its mean and variance from the training set.\n",
    "\n",
    "### 3. **Example: Independent Probabilities**:\n",
    "   - Consider an example where the probability of an airplane engine being unusually hot is 1/10, and the probability of high vibration is 1/20. The joint probability of both occurring is:\n",
    "     $$\\frac{1}{10} \\times \\frac{1}{20} = \\frac{1}{200}$$\n",
    "   This illustrates how multiplying probabilities reflects the likelihood of multiple events happening simultaneously.\n",
    "\n",
    "### 4. **Building the Anomaly Detection Model**:\n",
    "   - For each feature $x_j$, calculate its $\\mu_j$ and $\\sigma_j^2$. \n",
    "   - To calculate $p(x)$ for a new example $x$, use the Gaussian probability density function for each feature and multiply them together:\n",
    "     $$p(x_j) = \\frac{1}{\\sqrt{2 \\pi \\sigma_j^2}} e^{-\\frac{(x_j - \\mu_j)^2}{2 \\sigma_j^2}}$$\n",
    "   - If the resulting $p(x)$ is less than a threshold $\\epsilon$, the example is flagged as an anomaly.\n",
    "\n",
    "### 5. **Visualization Example**:\n",
    "   - In a dataset with two features $x_1$ and $x_2$, assume $\\mu_1 = 5, \\sigma_1 = 2$ and $\\mu_2 = 3, \\sigma_2 = 1$. This results in two Gaussian distributions, and when you multiply them, you get a 3D probability surface. Higher points on the surface represent more likely values for $x$.\n",
    "\n",
    "### 6. **Flagging Anomalies**:\n",
    "   - For example, consider two test points:\n",
    "     - $x_{\\text{test1}}$ near the center of the distribution gives $p(x_{\\text{test1}}) = 0.4$, which is higher than $\\epsilon = 0.02$, so it is not an anomaly.\n",
    "     - $x_{\\text{test2}}$, far from the center, gives $p(x_{\\text{test2}}) = 0.0021$, which is less than $\\epsilon$, so it is flagged as an anomaly.\n",
    "\n",
    "### 7. **Next Steps**:\n",
    "   - The next step involves choosing the threshold $\\epsilon$ and evaluating the performance of your anomaly detection system, which will be discussed in further detail.\n",
    "\n",
    "This method provides a systematic way of detecting anomalies by identifying unusual patterns in the feature space relative to a Gaussian distribution model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing and evaluating an anomaly detection system\n",
    "\n",
    "These practical tips focus on effectively developing an anomaly detection system, emphasizing how continuous evaluation during the development process can significantly improve system performance. Key takeaways include:\n",
    "\n",
    "1. **Real Number Evaluation**: It's crucial to have a way of evaluating the system during development. This can be done by associating real numbers with the system's performance as you adjust features or parameters (like epsilon). By having a metric to quickly measure improvement or degradation, decisions about changes become more efficient.\n",
    "\n",
    "2. **Labeled Data for Tuning**: Although anomaly detection typically deals with unlabeled data, having a small set of labeled anomalies (e.g., airplane engine defects) can significantly help. Label anomalous examples with $y = 1$ and normal ones with $y = 0$. These labels allow for cross-validation and test set creation.\n",
    "\n",
    "3. **Dataset Partitioning**: You can divide the dataset into training, cross-validation, and test sets. For example, you might train the system on 6,000 normal engines and use 2,000 normal and 10 anomalous engines for cross-validation and testing. This allows you to adjust the algorithm (e.g., tuning epsilon) to optimize the detection of anomalies.\n",
    "\n",
    "4. **Handling Few Anomalies**: If anomalies are rare, you can combine the remaining good and flawed engines into the cross-validation set instead of maintaining a separate test set. This is a trade-off, as it might lead to overfitting without a test set to validate final performance.\n",
    "\n",
    "5. **Performance Metrics**: Given the imbalance between normal and anomalous examples, standard accuracy metrics may not be suitable. Instead, use precision, recall, and F1 score, especially when the data distribution is skewed (i.e., far more normal examples than anomalies).\n",
    "\n",
    "6. **Tuning and Evaluation**: Evaluate the system on the cross-validation set by calculating the probability $p(x)$ for each example and adjusting epsilon to minimize false positives while correctly flagging anomalies. This method ensures that you are fine-tuning based on real performance data.\n",
    "\n",
    "7. **Supervised Learning Comparison**: While anomaly detection uses unsupervised learning, having labeled anomalies raises the question of why not use supervised learning. The next step would be to compare these approaches and understand when each method is preferable.\n",
    "\n",
    "This systematic approach ensures that even in the presence of sparse anomalies, your anomaly detection system can be fine-tuned to effectively distinguish between normal and faulty examples, providing robust performance in practical applications like manufacturing quality control or fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection vs. supervised learning\n",
    "\n",
    "This lecture provides a thoughtful comparison between anomaly detection and supervised learning, outlining key considerations for choosing the appropriate approach based on the nature of the data:\n",
    "\n",
    "1. **Data Characteristics**:\n",
    "   - **Anomaly Detection**: Best suited for situations with very few positive examples (0-20) and a larger number of negative examples. It focuses on modeling normal behavior (negative examples) and flags anything that deviates significantly as anomalous.\n",
    "   - **Supervised Learning**: More applicable when there are enough positive and negative examples. It assumes future positive examples will resemble those in the training set.\n",
    "\n",
    "2. **Types of Anomalies**:\n",
    "   - Anomaly detection excels when there are many potential types of anomalies or new, previously unseen forms. For instance, if aircraft engines might fail in unpredictable ways, anomaly detection can identify these novel failures.\n",
    "   - Supervised learning works well in more stable environments where patterns are consistent, like spam email detection, where spam messages tend to follow recognizable trends.\n",
    "\n",
    "3. **Use Case Examples**:\n",
    "   - **Fraud Detection**: Anomaly detection is used to spot new forms of fraud, while supervised learning focuses on known fraudulent patterns.\n",
    "   - **Email Spam**: Supervised learning is effective due to the relatively stable nature of spam characteristics over time.\n",
    "   - **Manufacturing**: Anomaly detection can identify new defects, while supervised learning is used for known defects (e.g., scratches on smartphones).\n",
    "\n",
    "4. **Security Applications**: Many security-related tasks utilize anomaly detection because attackers often find new methods that differ from previous patterns.\n",
    "\n",
    "5. **Practical Applications**: Weather prediction and medical diagnosis typically fall under supervised learning due to the repetitive nature of outputs.\n",
    "\n",
    "6. **Feature Importance**: In building anomaly detection algorithms, selecting and tuning the right features is crucial, which will be discussed in further detail in the next video.\n",
    "\n",
    "This framework helps determine when to employ each algorithm type based on the problem's specifics, the amount of data available, and the expected variety in the positive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing what features to use\n",
    "\n",
    "1. **When to Use Each Method**:\n",
    "   - **Anomaly Detection**: Best suited for scenarios with very few positive examples (0-20) and many negative examples. It focuses on modeling normal data and flags anything that deviates significantly as anomalous. Useful when you expect many potential anomalies or new types of anomalies that haven’t been seen before.\n",
    "   - **Supervised Learning**: Applicable when you have a sufficient number of both positive and negative examples. It assumes that future positive examples will resemble those in the training set.\n",
    "\n",
    "2. **Examples**:\n",
    "   - **Anomaly Detection**: Effective in detecting new types of fraud, machine failures, or defects in manufacturing that haven’t been encountered before.\n",
    "   - **Supervised Learning**: Works well for tasks like spam detection where the nature of the examples (spam emails) tends to remain relatively consistent over time.\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - In anomaly detection, the choice of features is critical because the algorithm relies solely on the data it learns from. If the features are not relevant or informative, the algorithm may fail to detect anomalies.\n",
    "   - In supervised learning, having irrelevant features can be managed since the algorithm can learn which features to ignore.\n",
    "\n",
    "### Tips for Feature Tuning in Anomaly Detection:\n",
    "1. **Transform Features**: Aim to make features more Gaussian in distribution. This can often improve the model's performance.\n",
    "   - Use transformations like:\n",
    "     - Logarithmic transformations (e.g., $$ \\log(X + C) $$)\n",
    "     - Square root or power transformations (e.g., $$ X^{0.5} $$ or $$ X^{0.4} $$)\n",
    "\n",
    "2. **Error Analysis**: After training, conduct error analysis on cross-validation results to identify where the algorithm fails to detect anomalies. This can inform the creation of new features that better capture anomalous behavior.\n",
    "\n",
    "3. **Combining Features**: Creating new features by combining existing ones can help highlight unusual patterns. For example, calculating the ratio of CPU load to network traffic can reveal anomalies that individual features may miss.\n",
    "\n",
    "### Conclusion:\n",
    "Choosing the right algorithm and carefully selecting and tuning features is essential for effective anomaly detection and supervised learning. Understanding the differences between these approaches can help in developing better models for specific applications. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
