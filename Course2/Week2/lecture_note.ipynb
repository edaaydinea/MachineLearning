{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation\n",
    "\n",
    "#### Overview\n",
    "This week’s focus is on training neural networks using your own data, building on last week's topic of inference. The example task is handwritten digit recognition (e.g., distinguishing between '0' and '1'). The neural network architecture has an input layer (the image), two hidden layers (25 and 15 units respectively), and one output unit.\n",
    "\n",
    "#### Key Steps for Training a Neural Network:\n",
    "1. **Specify the Model**:  \n",
    "   - Use TensorFlow's `Sequential` to create a neural network with three layers:\n",
    "     - First hidden layer: 25 units, sigmoid activation.\n",
    "     - Second hidden layer: 15 units, some activation function.\n",
    "     - Output layer: 1 unit (binary output).\n",
    "\n",
    "2. **Compile the Model**:  \n",
    "   - Define the loss function for training. For binary classification (e.g., recognizing a digit), the **binary crossentropy loss function** is used.\n",
    "   - Compile the model with the optimizer, the loss function, and performance metrics if needed.\n",
    "\n",
    "3. **Fit the Model**:  \n",
    "   - Use the `fit` function to train the model. It takes the dataset \\(X\\) (inputs) and \\(Y\\) (labels) and trains the model using gradient descent.\n",
    "   - **Epochs**: The number of steps to run the learning algorithm (e.g., gradient descent). Decide how many iterations (epochs) to run to optimize the model.\n",
    "\n",
    "#### Conceptual Understanding:\n",
    "- **Understanding Code**: It's crucial to understand each line of the TensorFlow code to effectively debug and improve models. If the learning algorithm doesn't perform well, this conceptual foundation will help troubleshoot.\n",
    "- **Mental Framework**: Understanding the roles of layers, loss functions, and optimization (like gradient descent) helps in diagnosing issues when training doesn’t yield expected results.\n",
    "\n",
    "---\n",
    "\n",
    "### Short Notes for Exam:\n",
    "1. **Neural Network Architecture**:\n",
    "   - Input → Hidden Layer 1 (25 units, sigmoid) → Hidden Layer 2 (15 units) → Output (1 unit).\n",
    "\n",
    "2. **TensorFlow Steps**:\n",
    "   - **Model Specification**: `Sequential` to string together layers.\n",
    "   - **Compile Model**: Use binary crossentropy for binary classification.\n",
    "   - **Fit Model**: Gradient descent optimization, specify number of **epochs** (iterations).\n",
    "\n",
    "3. **Loss Function**:\n",
    "   - **Binary Crossentropy**: For binary outcomes (e.g., digit recognition).\n",
    "\n",
    "4. **Epochs**: Control how long the model runs the training algorithm (more epochs = more training steps).\n",
    "\n",
    "5. **Debugging**: Understanding the internals of the model helps in debugging when the neural network doesn't work as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Step 1\n",
    "model = Sequential([\n",
    "    Dense(units = 25, activation=\"sigmoid\"),\n",
    "    Dense(units = 15, activation=\"sigmoid\"),\n",
    "    Dense(units = 1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Step 2\n",
    "model.compile(loss = BinaryCrossentropy())\n",
    "\n",
    "# Step 3\n",
    "model.fit(X,Y, epochs = 100) # epochs: number of steps in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "### Step-by-Step Breakdown:\n",
    "\n",
    "#### 1. **Specify the Model (Output Computation)**:\n",
    "   - In **logistic regression**, you predicted the output $ f(x) $ using the sigmoid function applied to the linear combination of parameters $ W $ and $ X $ plus the bias $ B $.\n",
    "     - $ f(x) = \\frac{1}{1 + e^{-z}} $, where $ z = W \\cdot X + B $\n",
    "   - In **neural networks**, this step corresponds to specifying the architecture of the network (layers, units per layer, activation functions). The layers and activations now define how input $ X $ is transformed through multiple layers of computation:\n",
    "     - **Input → Hidden Layer 1 (25 units, sigmoid) → Hidden Layer 2 (15 units) → Output (1 unit, sigmoid)**\n",
    "\n",
    "#### 2. **Define Loss and Cost Function**:\n",
    "   - In logistic regression, the **loss function** measured how well the model performed on a single training example. You used the **log loss (binary cross-entropy)**:\n",
    "     - $ \\text{Loss} = -y \\log(f(x)) - (1 - y) \\log(1 - f(x)) $\n",
    "   - The **cost function** was the average loss over all training examples, calculated to minimize over the entire dataset.\n",
    "     - $ J(W, B) = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Loss}(f(x^{(i)}), y^{(i)}) $\n",
    "   - For **neural networks**, the loss function is still **binary cross-entropy** for binary classification problems. TensorFlow provides this built-in, and once you specify the loss function, the cost function is automatically handled by taking the average over all training examples.\n",
    "\n",
    "#### 3. **Minimize the Cost Function**:\n",
    "   - For logistic regression, **gradient descent** updated the weights $ W $ and biases $ B $ by calculating the gradient of the cost function with respect to these parameters:\n",
    "     - $ W = W - \\alpha \\frac{\\partial J(W, B)}{\\partial W} $\n",
    "     - $ B = B - \\alpha \\frac{\\partial J(W, B)}{\\partial B} $\n",
    "   - Similarly, in neural networks, **gradient descent** or its variants (like Adam or RMSprop) is used to minimize the cost function by updating all the parameters (weights and biases) across all layers.\n",
    "   - The process of computing the gradients for each parameter is handled by TensorFlow’s **backpropagation** algorithm.\n",
    "\n",
    "#### How TensorFlow Implements These Steps:\n",
    "- **Step 1: Model Specification**  \n",
    "   You define the architecture of the neural network:\n",
    "   ```python\n",
    "   model = Sequential([\n",
    "       Dense(25, activation='sigmoid'),  # First hidden layer with 25 units\n",
    "       Dense(15, activation='sigmoid'),  # Second hidden layer with 15 units\n",
    "       Dense(1, activation='sigmoid')    # Output layer for binary classification\n",
    "   ])\n",
    "   ```\n",
    "- **Step 2: Compile the Model (Loss and Optimizer)**  \n",
    "   You specify the loss function (binary cross-entropy for classification) and the optimization method (e.g., Adam, which is often more efficient than gradient descent):\n",
    "   ```python\n",
    "   model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "   ```\n",
    "   - TensorFlow also handles other loss functions, such as **mean squared error** for regression tasks.\n",
    "  \n",
    "- **Step 3: Train the Model (Minimization of Cost Function)**  \n",
    "   Once the model is defined and compiled, you can train it using the `fit()` function. TensorFlow implements backpropagation to compute the gradients and updates the weights and biases accordingly.\n",
    "   ```python\n",
    "   model.fit(X_train, y_train, epochs=100)\n",
    "   ```\n",
    "\n",
    "### Key Concepts:\n",
    "- **Backpropagation**: TensorFlow automatically computes gradients through all layers using backpropagation, which allows for efficient weight updates in multi-layer networks.\n",
    "- **Optimization Algorithms**: While logistic regression uses plain gradient descent, TensorFlow supports more advanced optimizers like Adam, which are better suited for complex networks.\n",
    "- **Epochs**: An epoch is a single pass over the entire training dataset. You specify how many epochs (iterations) the training process should run.\n",
    "\n",
    "### Comparison with Logistic Regression:\n",
    "| Step                         | Logistic Regression                                                                 | Neural Network (Multilayer Perceptron)                             |\n",
    "|------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------|\n",
    "| **Step 1: Compute Output**    | $ f(x) = \\text{sigmoid}(W \\cdot X + B) $                                          | Forward pass through multiple layers using activations (e.g., sigmoid) |\n",
    "| **Step 2: Loss Function**     | Binary cross-entropy loss: $ -y \\log(f(x)) - (1 - y) \\log(1 - f(x)) $              | Same binary cross-entropy, or other losses depending on the task   |\n",
    "| **Step 3: Gradient Descent**  | Update $ W $ and $ B $ using gradients of the cost function                     | Backpropagation updates weights/biases for all layers              |\n",
    "\n",
    "### Final Thoughts:\n",
    "With TensorFlow, the manual work you did for logistic regression—computing gradients, updating weights, calculating losses—is now abstracted. Libraries like TensorFlow allow you to define, compile, and train complex neural networks with just a few lines of code, making them more accessible for rapid development. Understanding the underlying steps (as you did with logistic regression) still provides a valuable foundation for debugging and model optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activaction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to the Sigmoid Activation\n",
    "\n",
    "In neural networks, the choice of activation function plays a crucial role in determining the network's performance. So far, you’ve been using the **sigmoid activation function**, which is a smooth, S-shaped function. The sigmoid function outputs values between 0 and 1, making it suitable for binary classification tasks. However, when you need a more complex or scalable model, you can explore other activation functions, such as **ReLU** (Rectified Linear Unit) and **linear** activation, which can make your network more powerful.\n",
    "\n",
    "### **ReLU Activation Function (Rectified Linear Unit)**\n",
    "ReLU is defined as:\n",
    "$ g(z) = \\max(0, z) $\n",
    "\n",
    "- **Behavior**: For any input \\( z \\), if \\( z \\) is less than 0, ReLU outputs 0. Otherwise, it outputs the value of \\( z \\). This creates a linear relationship for positive values and a flat response for negative ones.\n",
    "- **Advantages**: ReLU helps overcome the vanishing gradient problem often encountered with sigmoid functions, especially in deeper networks. It also allows the activation to be any non-negative value, which can be useful when the quantity you're modeling can vary significantly.\n",
    "- **Usage**: ReLU is now the most commonly used activation function in hidden layers because of its simplicity and efficiency in training large neural networks.\n",
    "\n",
    "### **Linear Activation Function**\n",
    "Linear activation is defined as:\n",
    "$ g(z) = z $\n",
    "\n",
    "- **Behavior**: This function outputs the value of \\( z \\) directly without any transformation.\n",
    "- **Usage**: Linear activations are typically used in the output layer for regression tasks, where you need a continuous range of outputs. Since it's equivalent to not using any activation function, it preserves the linearity of the input.\n",
    "\n",
    "### **Choosing Between Activation Functions**\n",
    "- **Sigmoid**: Best for binary classification in the output layer (where you want probabilities between 0 and 1).\n",
    "- **ReLU**: Preferred for hidden layers in deep networks because of its ability to help models converge faster and handle non-negative values.\n",
    "- **Linear**: Typically used in the output layer when dealing with regression problems where you want unbounded continuous outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing activation functions\n",
    "\n",
    "In neural networks, choosing the right activation function for the **output layer** and the **hidden layers** is crucial for effective learning and making correct predictions. Here’s a structured approach to selecting the appropriate activation functions for different parts of your network:\n",
    "\n",
    "### **Output Layer:**\n",
    "\n",
    "1. **Binary Classification** (e.g., predicting whether something is true or false, like if a product is a top seller):\n",
    "   - **Sigmoid Activation**: \n",
    "     $$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "     - This function outputs a probability between 0 and 1, which is perfect for binary classification tasks. It’s the natural choice when the target label \\( y \\) is 0 or 1.\n",
    "  \n",
    "2. **Regression Tasks** (e.g., predicting continuous values like tomorrow’s stock price, which can be positive or negative):\n",
    "   - **Linear Activation**: \n",
    "     $$g(z) = z$$\n",
    "     - This function allows the output to take on any real number value, making it suitable for regression problems where the output can be both positive and negative.\n",
    "\n",
    "3. **Non-negative Continuous Values** (e.g., predicting the price of a house, which can’t be negative):\n",
    "   - **ReLU Activation**: \n",
    "     $$g(z) = \\max(0, z)$$\n",
    "     - This ensures that the output is always non-negative, which is useful when your target values are naturally constrained to be positive or zero.\n",
    "\n",
    "### **Hidden Layers:**\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "  $$g(z) = \\max(0, z)$$\n",
    "  - **Why ReLU?** ReLU is by far the most commonly used activation function in hidden layers due to several reasons:\n",
    "    - **Efficiency**: It is computationally simpler and faster than the sigmoid function.\n",
    "    - **Non-linearity**: ReLU introduces non-linearity, allowing the network to model complex patterns. The linear behavior for positive values helps the network learn more effectively, while the flat region for negative values keeps the gradient descent process stable.\n",
    "    - **Avoids the vanishing gradient problem**: Sigmoid functions can become flat at extreme values, which slows down learning due to tiny gradients. ReLU, by contrast, is only flat on one side, so it retains a large gradient for positive values.\n",
    "\n",
    "- **Leaky ReLU and other variants**:\n",
    "  - **Leaky ReLU** allows for a small slope on the negative side, preventing the \"dead ReLU\" problem where units can get stuck and never activate.\n",
    "  - **Swish** and other advanced activation functions might offer marginal improvements, but for most applications, ReLU is sufficient.\n",
    "\n",
    "### **Why Not Just Use Linear Activation Everywhere?**\n",
    "- **Without activation functions**, the network becomes just a series of linear combinations, and a network without non-linearity (like a ReLU) can only model linear relationships. To model complex, non-linear patterns, activation functions like ReLU are essential.\n",
    "  \n",
    "### **Summary of Activation Function Recommendations:**\n",
    "- **Output Layer**:\n",
    "  - Binary classification: **Sigmoid**\n",
    "  - Regression (positive and negative values): **Linear**\n",
    "  - Non-negative continuous values: **ReLU**\n",
    "  \n",
    "- **Hidden Layers**: **ReLU** as the default for most applications.\n",
    "\n",
    "This strategy allows you to tailor the network's behavior to the nature of your problem, ensuring that you can model both complex non-linear relationships and appropriate output ranges effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need activation functions?\n",
    "\n",
    "Neural networks rely on **non-linear activation functions** to enable them to model complex, non-linear patterns. If a neural network used **only linear activation functions**, it would not gain any additional modeling power over traditional **linear regression**. Here’s why:\n",
    "\n",
    "### **What Happens Without Non-Linear Activation Functions?**\n",
    "Let’s break it down using a simplified neural network example:\n",
    "\n",
    "1. **Single hidden layer example**:\n",
    "   - Consider an input $x$, a hidden layer with parameters $w_1$ and $b_1$, and an output layer with parameters $w_2$ and $b_2$.\n",
    "   - If the activation function is **linear** (i.e., $g(z) = z$) at both layers:\n",
    "     - The hidden layer output $a_1$ is just:\n",
    "       $$a_1 = w_1 \\cdot x + b_1$$\n",
    "     - The output layer output $a_2$ becomes:\n",
    "       $$a_2 = w_2 \\cdot a_1 + b_2 = w_2 \\cdot (w_1 \\cdot x + b_1) + b_2$$\n",
    "     - Simplifying this expression:\n",
    "       $$a_2 = (w_2 \\cdot w_1) \\cdot x + (w_2 \\cdot b_1 + b_2)$$\n",
    "     - Notice this is just a **linear function** of $x$, $a_2 = w \\cdot x + b$, where $w = w_2 \\cdot w_1$ and $b = w_2 \\cdot b_1 + b_2$.\n",
    "   \n",
    "   **Conclusion**: This output is no different from what you would get with **linear regression**—the entire point of using a neural network is defeated because the model can only learn a **linear relationship** between input and output.\n",
    "\n",
    "2. **Multiple layers with linear activation**:\n",
    "   - Even if you stack more layers, using only linear activation functions in all layers results in the same problem. A **linear function of a linear function** is still a **linear function**. Hence, multiple linear layers do not make the model more expressive or capable of learning non-linear patterns.\n",
    "   \n",
    "   **General Rule**: A neural network with linear activation functions in all layers is equivalent to a single-layer linear model.\n",
    "\n",
    "### **Why Use Non-Linear Activation Functions?**\n",
    "Non-linear activation functions like **ReLU** (Rectified Linear Unit) or **sigmoid** allow the network to capture complex, non-linear relationships between inputs and outputs, which is essential for most real-world tasks.\n",
    "\n",
    "- **ReLU** introduces non-linearity by outputting the input directly if it is positive, and 0 otherwise. This simple non-linearity enables the network to model complex patterns and makes gradient descent optimization more effective.\n",
    "  \n",
    "- Other non-linear functions like **sigmoid** (used in classification problems) or **tanh** (used sometimes in hidden layers) also introduce non-linearity in different ways, enabling the network to learn more expressive features.\n",
    "\n",
    "### **Key Insights:**\n",
    "- **Without non-linearity**, neural networks cannot learn complex patterns and reduce to linear models, which are insufficient for tasks like image recognition, speech processing, or any task requiring more than basic linear modeling.\n",
    "- **ReLU**, **sigmoid**, and **tanh** are essential because they enable **backpropagation** and gradient descent to work effectively, allowing the network to learn complex mappings from inputs to outputs.\n",
    "\n",
    "### **Summary**:\n",
    "- **Don’t use linear activation functions** in hidden layers. Always use non-linear activation functions like **ReLU**.\n",
    "- Neural networks need non-linear activation functions to differentiate them from linear regression and to make them powerful enough to learn complex relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass\n",
    "\n",
    "Multiclass classification involves problems where there are more than two possible output categories. Unlike binary classification, where the output $y$ takes one of two values (0 or 1), multiclass classification handles cases where $y$ can take several values.\n",
    "\n",
    "### Examples of Multiclass Classification:\n",
    "1. **Handwritten Digit Recognition**: Instead of just recognizing digits like 0 and 1, you may need to classify any digit from 0 to 9. This results in 10 possible output classes.\n",
    "2. **Disease Classification**: A medical application where a model must classify patients into one of several possible diseases, for instance, determining if a patient has one of five distinct conditions.\n",
    "3. **Defect Detection in Manufacturing**: In quality control, you might classify a product based on multiple defect types (e.g., scratches, discoloration, or chips in a pill).\n",
    "\n",
    "### Dataset Representation:\n",
    "In binary classification, you may have a dataset where features $x_1$ and $x_2$ are used to estimate the probability of $y$ being 0 or 1, often using logistic regression. However, in multiclass classification, the model must estimate the probabilities of $y$ taking on more than two values—such as classes 1, 2, 3, or 4.\n",
    "\n",
    "For example, imagine a dataset where:\n",
    "- Circles (O) represent one class.\n",
    "- X’s represent another class.\n",
    "- Triangles represent a third class.\n",
    "- Squares represent a fourth class.\n",
    "\n",
    "In this scenario, you’re not just predicting whether $y = 1$, but instead estimating the probability that $y$ equals each possible class (1, 2, 3, or 4).\n",
    "\n",
    "### Softmax Regression:\n",
    "To handle multiclass classification, logistic regression is generalized to **softmax regression**, which extends the idea of estimating a single probability into estimating probabilities for all possible classes. Instead of fitting a model to estimate the probability of $y = 1$, softmax regression helps in estimating the probabilities for all classes. \n",
    "\n",
    "In the next step, this **softmax function** is incorporated into a neural network, allowing the model to predict multiple categories from the input features. The softmax layer outputs a probability distribution over the possible classes, ensuring the predicted probabilities sum to 1, making it ideal for multiclass classification tasks. \n",
    "\n",
    "This approach allows neural networks to classify inputs into multiple categories effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "### Understanding Softmax Regression\n",
    "\n",
    "**Softmax Regression** extends logistic regression to handle multiclass classification problems where the output variable $y$ can take on more than two discrete values.\n",
    "\n",
    "#### How Softmax Regression Works\n",
    "\n",
    "1. **Computing Scores**:\n",
    "   For each possible class $j$, compute a score $z_j$:\n",
    "   $$z_j = w_j \\cdot x + b_j$$\n",
    "   where $w_j$ and $b_j$ are the parameters for class $j$, $x$ is the input feature vector, and $\\cdot$ denotes the dot product.\n",
    "\n",
    "2. **Softmax Function**:\n",
    "   Convert the scores into probabilities using the softmax function:\n",
    "   $$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^n e^{z_k}}$$\n",
    "   Here, $a_j$ represents the estimated probability that $y$ equals class $j$, and the denominator normalizes these probabilities so that they sum to 1.\n",
    "\n",
    "   For example, for four possible classes, you would compute:\n",
    "   $$a_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}}$$\n",
    "   $$a_2 = \\frac{e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}}$$\n",
    "   $$a_3 = \\frac{e^{z_3}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}}$$\n",
    "   $$a_4 = \\frac{e^{z_4}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}}$$\n",
    "\n",
    "   If the computed probabilities are $a_1 = 0.30$, $a_2 = 0.20$, $a_3 = 0.15$, the probability for the fourth class $a_4$ is:\n",
    "   $$a_4 = 1 - (a_1 + a_2 + a_3) = 1 - (0.30 + 0.20 + 0.15) = 0.35$$\n",
    "\n",
    "3. **General Case**:\n",
    "   For $n$ possible classes, the probability for class $j$ is:\n",
    "   $$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^n e^{z_k}}$$\n",
    "\n",
    "   The probabilities $a_1, a_2, \\ldots, a_n$ will always sum up to 1.\n",
    "\n",
    "#### Softmax Regression vs Logistic Regression\n",
    "\n",
    "When $n = 2$ (binary classification), softmax regression simplifies to logistic regression. The softmax function becomes equivalent to the sigmoid function used in logistic regression, thus allowing softmax regression to handle binary classification scenarios as well.\n",
    "\n",
    "#### Cost Function for Softmax Regression\n",
    "\n",
    "The cost function for softmax regression is the negative log-likelihood function. For a single training example with ground truth label $y = j$, the loss is:\n",
    "$$\\text{Loss} = -\\log(a_j)$$\n",
    "where $a_j$ is the predicted probability for the true class $j$.\n",
    "\n",
    "For multiple training examples, the cost function is the average of these individual losses over all training examples:\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\log(a_{y^{(i)}}^{(i)})$$\n",
    "where $a_{y^{(i)}}^{(i)}$ is the probability assigned to the true class for the $i$-th training example, and $m$ is the number of training examples.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Softmax Regression** generalizes logistic regression to multiple classes by computing probabilities for each class and normalizing them.\n",
    "- **Softmax Function** ensures the probabilities sum to 1, providing a valid probability distribution.\n",
    "- **Cost Function** for softmax regression incentivizes the model to assign high probabilities to the true class, effectively learning to predict the correct class with high confidence.\n",
    "\n",
    "In the next steps, this softmax regression model can be incorporated into a neural network to handle more complex multiclass classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Softmax Output\n",
    "\n",
    "### Building a Neural Network with Softmax Output Layer\n",
    "\n",
    "To extend neural networks for multiclass classification problems (e.g., digit recognition with 10 classes), you use a Softmax output layer. Here's a step-by-step explanation of how to incorporate the Softmax layer into a neural network and how to implement it using TensorFlow.\n",
    "\n",
    "#### Softmax Output Layer in a Neural Network\n",
    "\n",
    "1. **Architecture for Multiclass Classification**:\n",
    "   - **Hidden Layers**: Compute activations just as in any standard neural network.\n",
    "   - **Output Layer**: Replace the output layer with a Softmax layer that has as many units as the number of classes (e.g., 10 units for digit classification from 0 to 9).\n",
    "\n",
    "2. **Forward Propagation**:\n",
    "   - Compute the scores $z_j$ for each class $j$ from the output of the previous layer:\n",
    "     $$z_j = W_j \\cdot a_{(L-1)} + b_j$$\n",
    "     where $W_j$ and $b_j$ are the weights and biases for class $j$, and $a_{(L-1)}$ is the activation vector from the previous layer.\n",
    "\n",
    "   - Apply the Softmax function to convert scores into probabilities:\n",
    "     $$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^n e^{z_k}}$$\n",
    "     Here, $a_j$ is the probability of the input belonging to class $j$.\n",
    "\n",
    "   - The Softmax activation function ensures that the sum of probabilities $a_1, a_2, \\ldots, a_n$ is 1.\n",
    "\n",
    "3. **Unique Property of Softmax**:\n",
    "   - Unlike other activation functions (sigmoid, ReLU), the Softmax function considers all class scores simultaneously. Each output probability $a_j$ depends on all the scores $z_1, z_2, \\ldots, z_n$.\n",
    "\n",
    "#### Implementing in TensorFlow\n",
    "\n",
    "1. **Define the Model**:\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "\n",
    "   # Define the model architecture\n",
    "   model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Dense(25, activation='relu', input_shape=(input_dim,)),  # Hidden layer 1\n",
    "       tf.keras.layers.Dense(15, activation='relu'),                           # Hidden layer 2\n",
    "       tf.keras.layers.Dense(10, activation='softmax')                         # Output layer with Softmax\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "2. **Compile the Model**:\n",
    "   - Use `SparseCategoricalCrossentropy` as the loss function for multiclass classification:\n",
    "     ```python\n",
    "     model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "     ```\n",
    "\n",
    "3. **Train the Model**:\n",
    "   - Fit the model to your data:\n",
    "     ```python\n",
    "     model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "     ```\n",
    "\n",
    "   Here, `x_train` is your input data and `y_train` is your corresponding labels (with values ranging from 0 to 9 for digit classification).\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- **SparseCategoricalCrossentropy**: Used because the target values are integers representing the class labels.\n",
    "- **Softmax Activation**: Ensures that the model outputs a probability distribution over the classes.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Softmax Layer**: Converts raw scores into probabilities for multiclass classification.\n",
    "- **TensorFlow Implementation**: Define a neural network with a Softmax output layer, compile with `SparseCategoricalCrossentropy`, and train as usual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved implementation of softmax\n",
    "\n",
    "This lecture explains how to improve the numerical stability of the softmax regression model in TensorFlow by avoiding potential round-off errors that occur when computing values like the softmax activations or loss functions.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Numerical Stability and Round-off Errors**: \n",
    "    - When computing values in a computer, round-off errors can occur because the computer has limited memory to store numbers (floating-point numbers).\n",
    "    - Example: Computing $x = \\frac{2}{10,000}$ directly versus computing it through intermediate steps like $1 + \\frac{1}{10,000} - (1 - \\frac{1}{10,000})$ may lead to small discrepancies due to round-off errors, especially when handling very small or large numbers.\n",
    "\n",
    "2. **Softmax Layer and Loss Function**:\n",
    "    - In softmax regression, activations are computed as $a_j = \\frac{e^{z_j}}{\\sum_{i=1}^{10} e^{z_i}}$, where $z_j$ is the output of the last linear layer.\n",
    "    - Instead of computing softmax activations and loss in separate steps (which can lead to numerical issues), TensorFlow allows you to directly compute the loss function using more numerically stable methods.\n",
    "\n",
    "3. **Improved Implementation Using `from_logits=True`**:\n",
    "    - TensorFlow has an option to directly compute the loss from the raw logits (the $z$ values) without first computing the softmax activations. This is done using the `from_logits=True` parameter.\n",
    "    - By avoiding the explicit calculation of the softmax activations and instead folding it into the loss calculation, TensorFlow can rearrange computations in a more numerically stable way, reducing round-off errors.\n",
    "\n",
    "4. **Logits**: \n",
    "    - In machine learning, logits refer to the raw output of the final linear layer (before applying softmax or any other transformation). When using `from_logits=True`, TensorFlow assumes you're working with logits and internally applies the necessary transformations to calculate the loss.\n",
    "\n",
    "5. **Application in Multi-Class Classification**:\n",
    "    - The updated TensorFlow code no longer requires explicitly specifying the softmax activation in the final layer. Instead, it can directly calculate the logits and handle everything through the loss function.\n",
    "    - The cost function (e.g., `SparseCategoricalCrossentropy`) automatically applies the softmax internally when `from_logits=True` is used.\n",
    "\n",
    "### Benefits of the Improved Approach:\n",
    "- **Reduced Numerical Errors**: Avoids extremely large or small intermediate values (e.g., $e^{z}$) that could introduce inaccuracies.\n",
    "- **Cleaner Code**: Handles both activation and loss calculation in one step through the loss function specification.\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(10)  # No softmax activation here\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "```\n",
    "\n",
    "This approach improves numerical stability, although the code might seem less intuitive. It is the preferred method when dealing with multi-class classification problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Neural Network Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimization\n",
    "\n",
    "The **Adam optimization algorithm** improves upon standard gradient descent by adapting the learning rate for each parameter during training. This leads to faster convergence and more efficient learning, especially for neural networks. Here's a breakdown of the key points:\n",
    "\n",
    "### Gradient Descent Recap:\n",
    "- Gradient descent updates parameters (e.g., $w_j $) using:\n",
    "  $$\n",
    "  w_j = w_j - \\alpha \\cdot \\frac{\\partial J}{\\partial w_j}\n",
    "  $$\n",
    "  where $\\alpha $ is the learning rate, and $\\frac{\\partial J}{\\partial w_j} $ is the gradient of the cost function $J $ with respect to $w_j $.\n",
    "\n",
    "### Issues with Fixed Learning Rate:\n",
    "- **Small learning rates** can result in slow progress, as each step barely moves toward the minimum.\n",
    "- **Large learning rates** can cause oscillations, making it hard to converge smoothly to the minimum.\n",
    "\n",
    "### The Adam Algorithm (Adaptive Moment Estimation):\n",
    "- **Adaptively changes the learning rate** for each parameter individually.\n",
    "- **Adjusts learning rates** based on how the gradient behaves:\n",
    "  - If a parameter continues moving in the same direction, Adam **increases the learning rate** for that parameter, allowing it to converge faster.\n",
    "  - If a parameter oscillates (bounces back and forth), Adam **reduces the learning rate** for smoother convergence.\n",
    "  \n",
    "### Key Benefits:\n",
    "- **Momentum and adaptive learning rates:** Adam keeps track of past gradients (momentum) and adjusts the learning rates based on that information.\n",
    "- **Numerical stability:** Adam reduces the need to tune the learning rate manually and is robust to different initial settings.\n",
    "  \n",
    "### Implementation in Code:\n",
    "In TensorFlow/Keras, implementing Adam is straightforward:\n",
    "```python\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "Here, the learning rate is set to $10^{-3} $, but this can be adjusted based on the problem.\n",
    "\n",
    "### Why Use Adam?\n",
    "- **Faster convergence:** Adam dynamically adjusts learning rates, often resulting in quicker and more efficient training than standard gradient descent.\n",
    "- **De facto standard:** It's widely used in practice due to its ability to handle large datasets and complex models effectively.\n",
    "\n",
    "In summary, the Adam optimizer is a more advanced and adaptive version of gradient descent, making it ideal for training neural networks efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Layer Types\n",
    "\n",
    "This lecture introduces **convolutional layers**, a fundamental building block of **convolutional neural networks (CNNs)**, which differ from the **dense layers** previously discussed in neural networks.\n",
    "\n",
    "### Recap of Dense Layers:\n",
    "- In a dense layer, each neuron in the hidden layer is connected to every neuron in the previous layer. This works well but can be computationally expensive, especially for large input data like images or signals.\n",
    "\n",
    "### Convolutional Layers:\n",
    "- **Convolutional layers** operate differently. Instead of each neuron looking at all inputs from the previous layer, neurons in a convolutional layer focus on **small, localized regions** (called **receptive fields**) of the input.\n",
    "  \n",
    "#### Example with Handwritten Digit:\n",
    "- In an image (e.g., a handwritten digit), each neuron in the convolutional layer looks only at a small portion of the image (a small rectangle of pixels), not the entire image.\n",
    "- By using these localized connections, convolutional layers:\n",
    "  - **Speed up computation**, as fewer connections are made compared to dense layers.\n",
    "  - **Reduce overfitting**, since they require fewer parameters and learn to capture local patterns.\n",
    "\n",
    "### Convolutional Neural Networks (CNNs):\n",
    "- Multiple convolutional layers can be stacked to form a **convolutional neural network**. CNNs are especially effective for tasks like **image recognition** and **signal classification**.\n",
    "- In each convolutional layer, neurons focus on different portions of the input, enabling the network to capture **local patterns** like edges or specific features in an image.\n",
    "\n",
    "#### Example with EKG Signals:\n",
    "- For a one-dimensional signal like an **EKG** (electrocardiogram), neurons in a convolutional layer only focus on small windows of the signal, analyzing a subset of the data points at a time.\n",
    "- Subsequent layers can also be convolutional, continuing to look at smaller sections of the activations from the previous layer.\n",
    "\n",
    "### Practical Benefits:\n",
    "- **Efficiency**: By focusing on smaller regions of the input, CNNs can process data faster and with fewer parameters than dense layers.\n",
    "- **Better Generalization**: By limiting the number of parameters, CNNs are less prone to overfitting, making them more effective when training with limited data.\n",
    "  \n",
    "### Architecture Choices in CNNs:\n",
    "- Researchers have flexibility in choosing the size of the **window** (receptive field) each neuron looks at, as well as the number of neurons in each layer, allowing for various CNN designs suited to specific tasks.\n",
    "\n",
    "### Neural Network Layer Types:\n",
    "- The video highlights how new layer types (like **convolutional layers**) add flexibility and power to neural networks.\n",
    "- Other types of layers, such as **transformers, LSTMs,** or **attention models**, represent ongoing research efforts to develop more effective neural networks by combining different kinds of layers.\n",
    "\n",
    "### Conclusion:\n",
    "- While dense layers can be powerful, **convolutional layers** offer advantages for tasks involving structured data like images and signals. Understanding different types of layers broadens the capacity of neural networks to solve a wide range of problems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
