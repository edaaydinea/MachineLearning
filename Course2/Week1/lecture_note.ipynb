{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "**Overview:**\n",
    "This course covers neural networks (deep learning algorithms), decision trees, and practical advice on building machine learning systems. It focuses on making systematic decisions to optimize machine learning projects and avoid common pitfalls.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Topics by Week:**\n",
    "\n",
    "- **Week 1: Neural Networks & Inference**\n",
    "  - Introduction to neural networks and how they work.\n",
    "  - **Inference**: Using pre-trained neural network parameters to make predictions.\n",
    "  - Example: Downloading a neural network model and using it for prediction is called inference.\n",
    "\n",
    "- **Week 2: Training Neural Networks**\n",
    "  - Learn to train neural networks using labeled data (X, Y).\n",
    "  - Techniques for optimizing parameters and improving model performance.\n",
    "  \n",
    "- **Week 3: Practical Advice for Building Machine Learning Systems**\n",
    "  - Systematic decision-making for ML projects (e.g., collecting data vs. improving hardware).\n",
    "  - Tips to avoid common mistakes that can lead to inefficient project work.\n",
    "  - Efficiently building and scaling machine learning applications.\n",
    "\n",
    "- **Week 4: Decision Trees**\n",
    "  - Introduction to decision trees, a powerful and widely used algorithm.\n",
    "  - Comparison with neural networks: less hype, but still very effective for certain applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Practical advice on building ML systems is unique to this course and helps avoid costly mistakes.\n",
    "- Decision-making involves trade-offs like spending time on data collection vs. hardware upgrades.\n",
    "- Inference refers to using a trained neural network for prediction.\n",
    "- Neural networks are powerful, but decision trees are also important and widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons and the brain\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Neural Networks Origins:**\n",
    "  - Originally motivated by mimicking the biological brain.\n",
    "  - Early research began in the 1950s.\n",
    "  - Inspired by **neurons** in the brain, which send electrical impulses and form new connections.\n",
    "  - The artificial neuron model is simplified from the biological neuron.\n",
    "\n",
    "- **History Timeline:**\n",
    "  - **1950s:** Neural networks were first developed but then fell out of favor.\n",
    "  - **1980s-1990s:** Regained traction, especially in applications like handwritten digit recognition.\n",
    "  - **Late 1990s:** Neural networks lost popularity again.\n",
    "  - **2005 and beyond:** Neural networks re-emerged as **deep learning**, becoming a dominant force in AI.\n",
    "  - **Key Applications:** Speech recognition, computer vision (notably in 2012 with the **ImageNet moment**), natural language processing, climate change, medical imaging, and many more.\n",
    "\n",
    "---\n",
    "\n",
    "**Biological vs. Artificial Neurons:**\n",
    "- **Biological Neurons:**\n",
    "  - Dendrites receive inputs, the cell body processes, and the axon sends outputs to other neurons.\n",
    "  - Electrical impulses form the basis of human thought.\n",
    "  \n",
    "- **Artificial Neurons:**\n",
    "  - Input is numbers, computations are performed, and outputs are generated for the next neuron.\n",
    "  - Artificial neural networks are vastly simplified compared to biological brains, and modern AI researchers rely more on **engineering principles** than biological motivations.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Neural Networks Took Off:**\n",
    "- **Data Availability:**\n",
    "  - The rise of digitized data (e.g., medical records, internet data, etc.) led to a surge in available data for training models.\n",
    "  - Traditional algorithms (e.g., logistic regression) couldn't scale well with large datasets.\n",
    "  \n",
    "- **Performance Improvements with Size:**\n",
    "  - Larger neural networks (more neurons) could take advantage of big data, resulting in improved performance for certain applications.\n",
    "  - Unlike traditional algorithms, neural networks improve with more data and model complexity.\n",
    "\n",
    "- **Hardware Advances:**\n",
    "  - Faster processors, especially **GPUs (Graphics Processing Units)**, enabled larger neural networks and deep learning algorithms to be trained efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaways:**\n",
    "- Neural networks have diverged significantly from their original biological inspiration, focusing more on practical applications.\n",
    "- The explosion of data and advances in hardware (GPUs) were key factors in the resurgence of deep learning since 2005.\n",
    "- Deep learning algorithms have revolutionized a wide range of fields by outperforming traditional machine learning methods when large datasets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand prediction\n",
    "\n",
    "#### 1. **Introduction to Neural Networks**\n",
    "   - **Example**: Demand prediction for a T-shirt (predicting if a product will be a top seller).\n",
    "   - **Input feature (x)**: Price of the T-shirt.\n",
    "   - **Logistic Regression**: Uses sigmoid function to predict probability $ a = \\frac{1}{1 + e^{-(wx + b)}} $.\n",
    "   - **Activation (a)**: Represents the neuron's output, a concept borrowed from neuroscience (refers to electrical activity in neurons).\n",
    "   - **Neural Network as a Model**: Simplified model of a biological neuron that processes inputs (e.g., price) and outputs probabilities.\n",
    "\n",
    "#### 2. **Building a Neural Network**\n",
    "   - **Complex Example**: Using multiple features to predict if a T-shirt is a top seller:\n",
    "     - Features: Price, shipping cost, marketing, material quality.\n",
    "     - Factors affecting sales: Affordability, awareness, and perceived quality.\n",
    "     - **Neurons**: \n",
    "       - 1st Neuron: Predicts affordability (based on price and shipping costs).\n",
    "       - 2nd Neuron: Predicts awareness (based on marketing).\n",
    "       - 3rd Neuron: Predicts perceived quality (based on price and material).\n",
    "   - **Layering**:\n",
    "     - Group neurons into **layers**.\n",
    "     - **Input Layer**: Contains raw features.\n",
    "     - **Hidden Layer**: Processes the features into activation values.\n",
    "     - **Output Layer**: Outputs the final prediction (e.g., probability of being a top seller).\n",
    "\n",
    "#### 3. **Neural Network Architecture**\n",
    "   - **Fully Connected**: Each neuron in a layer receives input from all neurons in the previous layer.\n",
    "   - **Vector Representation**: \n",
    "     - Inputs (features) are represented as vectors.\n",
    "     - Hidden layer outputs a vector of activations, which is used by the next layer.\n",
    "   - **Hidden Layer**:\n",
    "     - Takes in raw input features and computes activations (e.g., affordability, awareness, and quality).\n",
    "     - Called \"hidden\" because the training data only provides inputs and outputs (not the intermediate activations).\n",
    "   - **Key Insight**: Neural networks learn their own features (like affordability) during training, so manual feature engineering is not needed.\n",
    "\n",
    "#### 4. **Multilayer Perceptron (MLP)**\n",
    "   - **Deep Neural Networks**: Neural networks with multiple hidden layers.\n",
    "   - **Example**: \n",
    "     - First hidden layer takes input features and computes activations.\n",
    "     - Second hidden layer takes activations from the first layer and outputs new activations.\n",
    "     - **Architecture**: You need to decide how many hidden layers and how many neurons per layer.\n",
    "     - The right architecture can improve model performance.\n",
    "\n",
    "#### 5. **Terminology & Concepts**\n",
    "   - **Activation**: Output of a neuron (e.g., probability of being a top seller).\n",
    "   - **Layer Types**:\n",
    "     - **Input Layer**: Initial features.\n",
    "     - **Hidden Layer**: Processes inputs and outputs intermediate values.\n",
    "     - **Output Layer**: Final prediction.\n",
    "   - **Multilayer Perceptron (MLP)**: A neural network with multiple hidden layers.\n",
    "   - **Training Set**: Provides the inputs and outputs (but not intermediate hidden layer values).\n",
    "   - **Fully Connected Layers**: Neurons in each layer receive inputs from every neuron in the previous layer.\n",
    "\n",
    "#### 6. **Feature Learning**\n",
    "   - Neural networks **learn their own features** (e.g., affordability, quality) during training, eliminating the need for manual feature engineering.\n",
    "   - This ability to learn relevant features is what makes neural networks powerful learning algorithms.\n",
    "\n",
    "#### 7. **Architecture Considerations**\n",
    "   - **Hidden Layers**: Decide how many hidden layers and how many neurons per layer.\n",
    "   - More layers can capture complex patterns, but can also increase computational complexity.\n",
    "   - **Architecture**: The design of the neural network (layers, neurons per layer) influences performance.\n",
    "\n",
    "### Key Notes for Studying:\n",
    "   - **Sigmoid Function**: Key to logistic regression for binary classification.\n",
    "   - **Activation**: Think of it as a neuron's output, analogous to a biological neuron’s signal.\n",
    "   - **Hidden Layers**: The intermediate layers between input and output; they compute features that are not visible in training data.\n",
    "   - **Multilayer Perceptron**: Term for neural networks with more than one hidden layer.\n",
    "   - **Fully Connected**: Each neuron in a layer connects to all neurons in the previous layer.\n",
    "   - **Key Decision Points**: Choosing the number of hidden layers and neurons per layer impacts the neural network’s performance.\n",
    "   - **Feature Learning**: Neural networks can automatically discover important features during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Recognizing Images\n",
    "\n",
    "**Overview:**\n",
    "- Neural networks can be used for tasks like **face recognition** by processing images as pixel grids (e.g., 1000x1000 pixels).\n",
    "- The input image is converted into a **feature vector** by unrolling the pixel matrix (1,000 x 1,000 pixels = 1 million pixel intensity values).\n",
    "- The neural network extracts features through **multiple hidden layers** and ultimately predicts the identity of the person in the image.\n",
    "\n",
    "---\n",
    "\n",
    "**Detailed Concepts:**\n",
    "\n",
    "1. **Image Representation:**\n",
    "   - A 1000x1000 image is represented as a grid of pixel intensity values (e.g., 0-255).\n",
    "   - The image is unrolled into a **vector** of 1 million pixel intensity values as the input.\n",
    "\n",
    "2. **Neural Network Architecture:**\n",
    "   - The input feature vector (1 million values) is fed into the **input layer**.\n",
    "   - Multiple **hidden layers** process the input to extract features.\n",
    "   - The final **output layer** predicts the probability of the person’s identity.\n",
    "   \n",
    "3. **Feature Learning in Hidden Layers:**\n",
    "   - **First hidden layer:** Detects basic **features like edges** (e.g., vertical or horizontal lines).\n",
    "   - **Second hidden layer:** Groups edges to detect **parts of the face**, such as eyes, noses, or ears.\n",
    "   - **Third hidden layer:** Detects **coarse face shapes** by combining facial features.\n",
    "   - The network learns these features **automatically from data** without explicit instructions.\n",
    "\n",
    "4. **Hierarchical Feature Learning:**\n",
    "   - Early layers focus on small, fine-grained features (e.g., edges).\n",
    "   - Deeper layers capture larger, more complex patterns (e.g., face parts, car shapes).\n",
    "   - **Visualizations:** First-layer neurons focus on small image windows, while deeper layers analyze larger regions of the image.\n",
    "\n",
    "5. **Generalization to Different Data:**\n",
    "   - If trained on a different dataset (e.g., **car images**), the network adapts by detecting features relevant to the new task.\n",
    "   - The first layer still detects edges, while later layers detect **car parts** (e.g., wheels, windows) and full car shapes.\n",
    "\n",
    "6. **Key Insight:**\n",
    "   - Neural networks automatically learn to detect **different features** from the data they are trained on, whether it's faces or cars.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Terms:**\n",
    "- **Feature Vector:** A long list of pixel intensity values that represent the image.\n",
    "- **Hidden Layer:** Intermediate layers in a neural network where feature extraction happens.\n",
    "- **Neurons:** Computational units in each layer that learn patterns (edges, shapes) by adjusting weights.\n",
    "- **Feature Hierarchy:** Layers closer to the input learn simple features, while deeper layers learn more abstract, complex features.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps in Learning:**\n",
    "- **Mathematical details:** Next videos will focus on the specific **mathematics** and implementation of neural networks, including how to construct and train the layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Short Notes:**\n",
    "- **Input Representation:** Images are flattened into vectors (e.g., 1 million values for 1000x1000 images).\n",
    "- **Layer Function:** Early layers detect basic edges; deeper layers detect complex shapes (face parts, full faces).\n",
    "- **Feature Learning:** Neural networks learn features autonomously based on training data.\n",
    "- **Application Adaptation:** Same network architecture can be adapted to various tasks (e.g., faces, cars) by changing the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network layer\n",
    "\n",
    "#### 1. **Layer of Neurons:**\n",
    "   - Fundamental building block of modern neural networks.\n",
    "   - Example: A neural network with four input features, a hidden layer of three neurons, and one output neuron.\n",
    "   - **Hidden Layer Computation:**\n",
    "     - Inputs are sent to each neuron, which operates like a logistic regression unit.\n",
    "     - Each neuron has its own parameters $ w $ and $ b $, e.g., $ w_1 $, $ b_1 $ for the first neuron.\n",
    "     - Activation output $ a $ is computed using:\n",
    "       $$a_1 = g(w_1 \\cdot x + b_1)$$\n",
    "       - $ g(z) $ is the sigmoid function: $ g(z) = \\frac{1}{1 + e^{-z}} $.\n",
    "       - Activation outputs form a vector, e.g., $ a = [0.3, 0.7, 0.2] $.\n",
    "\n",
    "#### 2. **Layer Notation:**\n",
    "   - Input layer = Layer 0, hidden layers = Layer 1, Layer 2, etc.\n",
    "   - Use superscript notation to indicate different layers, e.g., $ a^{[1]} $, $ w^{[1]} $, $ b^{[1]} $ for parameters of Layer 1.\n",
    "\n",
    "#### 3. **Activation and Layers:**\n",
    "   - **Layer 1:** Computes activation values $ a^{[1]} $, which becomes the input to Layer 2.\n",
    "   - **Layer 2 (Output Layer):** Computes the final activation $ a^{[2]} $, where:\n",
    "     $$a^{[2]} = g(w_1^{[2]} \\cdot a^{[1]} + b_1^{[2]})$$\n",
    "   - The final output is a scalar value (e.g., $ 0.84 $) if the output layer has a single neuron.\n",
    "\n",
    "#### 4. **Binary Classification:**\n",
    "   - To predict a binary outcome, threshold the output $ a^{[2]} $ at 0.5.\n",
    "   - If $ a^{[2]} > 0.5 $, predict $ \\hat{y} = 1 $; otherwise, predict $ \\hat{y} = 0 $.\n",
    "\n",
    "#### 5. **Key Concepts:**\n",
    "   - Neural networks consist of layers where each neuron applies logistic regression to input values.\n",
    "   - Outputs from one layer become inputs for the next, ultimately producing a final prediction.\n",
    "   - Thresholding converts the final output into a binary decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex neural networks\n",
    "\n",
    "### Academic Summary: Complex Neural Networks and Layer Computation\n",
    "\n",
    "#### 1. **Neural Network Overview**\n",
    "- **Neural network layers**: Composed of multiple layers; conventionally, we do not count the input layer. A network with four layers typically refers to three hidden layers and one output layer, excluding the input layer.\n",
    "- **Layer types**:\n",
    "  - **Layer 0**: Input layer (not counted)\n",
    "  - **Layers 1-3**: Hidden layers\n",
    "  - **Layer 4**: Output layer\n",
    "\n",
    "#### 2. **Layer Computation Process**\n",
    "- **Layer structure**: Each layer takes an input vector, performs computations with parameters (weights and biases), and outputs an activation vector.\n",
    "- **Neurons (units)**: Each neuron in a layer has a weight vector (`w`) and a bias term (`b`).\n",
    "  - Example for Layer 3 with 3 neurons:\n",
    "    - $ a_1 = \\text{sigmoid}(w_1 \\cdot a_2 + b_1) $\n",
    "    - $ a_2 = \\text{sigmoid}(w_2 \\cdot a_2 + b_2) $\n",
    "    - $ a_3 = \\text{sigmoid}(w_3 \\cdot a_2 + b_3) $\n",
    "- **Activation vector**: After applying the activation function (sigmoid here), the layer outputs a vector of activations $ a_3 = [a_1, a_2, a_3] $.\n",
    "\n",
    "#### 3. **Notation Details**\n",
    "- **Subscripts**: Denote the index of neurons (e.g., $ a_3^2 $ refers to the 2nd neuron in Layer 3).\n",
    "- **Superscripts**: Indicate the layer number (e.g., $ w_1^{[3]} $ refers to the weights of the 1st neuron in Layer 3).\n",
    "- **Inputs and Outputs**:\n",
    "  - **Input to a layer**: The activation output from the previous layer. For Layer 3, the input is $ a_2 $ (output of Layer 2).\n",
    "  - **Output of a layer**: The activations after applying the activation function (e.g., sigmoid).\n",
    "\n",
    "#### 4. **Generalized Layer Computation**\n",
    "- **Equation for any layer** $ l $ and any unit $ j $:\n",
    "  - $ a_j^{[l]} = \\text{sigmoid}(w_j^{[l]} \\cdot a^{[l-1]} + b_j^{[l]}) $\n",
    "  - $ w_j^{[l]} $ refers to the weight vector for unit $ j $ in layer $ l $.\n",
    "  - $ a^{[l-1]} $ is the activation vector from the previous layer (layer $ l-1 $).\n",
    "\n",
    "#### 5. **Activation Function**\n",
    "- **Sigmoid function**: A common activation function used in neural networks:\n",
    "  - $ g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "  - Transforms the weighted sum of inputs into an activation value between 0 and 1.\n",
    "- **Other activation functions**: While sigmoid is a classic choice, other functions can be used in place of $ g $, such as ReLU or Tanh, which will be explored later.\n",
    "\n",
    "#### 6. **Special Case: Input Layer**\n",
    "- **Input vector**: Denoted as $ X $ and referred to as $ a_0 $.\n",
    "  - For the first layer ($ l = 1 $), the activation is computed as:\n",
    "    - $ a_1 = \\text{sigmoid}(w_1 \\cdot a_0 + b_1) $\n",
    "  - This ensures consistent notation across layers.\n",
    "\n",
    "#### 7. **Inference Algorithm for Prediction**\n",
    "- **Forward propagation**: The process of computing the activations layer by layer, starting from the input and moving to the output.\n",
    "- **Prediction**: At the final layer (output), the computed activations represent the network's prediction.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Layer-by-layer computation**: Each hidden/output layer computes activations based on the activations from the previous layer.\n",
    "- **Weights and biases**: Parameters are updated through training, but their role in each layer is critical for transforming inputs into meaningful outputs.\n",
    "- **Activation functions**: These introduce non-linearity, allowing the network to learn complex patterns.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: making predictions (forward propagation)\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Forward Propagation**: Algorithm that allows a neural network to make inferences/predictions. It computes the output from inputs by passing values through the layers of the network, using learned weights and biases.\n",
    "- **Binary Classification**: For this example, we classify handwritten digits (0 vs. 1). The input is an 8x8 image with 64 pixel intensity values (0 to 255, representing shades from black to white).\n",
    "\n",
    "#### Neural Network Architecture:\n",
    "1. **Input Layer (Layer 0)**: \n",
    "   - 64 features (from 8x8 image matrix).\n",
    "   - Input is denoted as $ x $ (or $ a_0 $, the activation of layer 0).\n",
    "   \n",
    "2. **First Hidden Layer (Layer 1)**:\n",
    "   - **25 neurons**.\n",
    "   - Computes $ a_1 $ using the formula:  \n",
    "     $ z_1 = W_1 \\cdot x + b_1 $  \n",
    "     $ a_1 = \\sigma(z_1) $, where $ \\sigma $ is the activation function (e.g., sigmoid).\n",
    "   \n",
    "3. **Second Hidden Layer (Layer 2)**:\n",
    "   - **15 neurons**.\n",
    "   - Computes $ a_2 $ using the formula:  \n",
    "     $ z_2 = W_2 \\cdot a_1 + b_2 $  \n",
    "     $ a_2 = \\sigma(z_2) $.\n",
    "   \n",
    "4. **Output Layer (Layer 3)**:\n",
    "   - **1 neuron** (binary output: 0 or 1).\n",
    "   - Computes $ a_3 $ using the formula:  \n",
    "     $ z_3 = W_3 \\cdot a_2 + b_3 $  \n",
    "     $ a_3 = \\sigma(z_3) $, where $ a_3 $ is a scalar value representing the predicted probability of the digit being 1.\n",
    "   - **Prediction**: $ a_3 > 0.5 $ predicts digit 1, otherwise 0.\n",
    "\n",
    "#### Mathematical Steps of Forward Propagation:\n",
    "1. **From Input to Layer 1**:\n",
    "   - $ z_1 = W_1 \\cdot x + b_1 $\n",
    "   - $ a_1 = \\sigma(z_1) $\n",
    "   \n",
    "2. **From Layer 1 to Layer 2**:\n",
    "   - $ z_2 = W_2 \\cdot a_1 + b_2 $\n",
    "   - $ a_2 = \\sigma(z_2) $\n",
    "   \n",
    "3. **From Layer 2 to Output**:\n",
    "   - $ z_3 = W_3 \\cdot a_2 + b_3 $\n",
    "   - $ a_3 = \\sigma(z_3) $, where $ a_3 $ is the final predicted probability.\n",
    "\n",
    "4. **Binary Classification**:  \n",
    "   - If $ a_3 > 0.5 $, classify as 1, otherwise classify as 0.\n",
    "\n",
    "#### Additional Notes:\n",
    "- **Activation Function**: The activation function $ \\sigma $ is often a sigmoid or ReLU. In this example, it’s implied to be a sigmoid for simplicity.\n",
    "- **Weights and Biases**: These are learned during training (using an algorithm called **backpropagation**, covered in future lessons).\n",
    "- **Network Architecture**: The architecture tapers down (more neurons in early layers, fewer towards output) – a common design choice.\n",
    "\n",
    "#### Summary of Key Formulas:\n",
    "1. $ z_1 = W_1 \\cdot x + b_1 $\n",
    "2. $ a_1 = \\sigma(z_1) $\n",
    "3. $ z_2 = W_2 \\cdot a_1 + b_2 $\n",
    "4. $ a_2 = \\sigma(z_2) $\n",
    "5. $ z_3 = W_3 \\cdot a_2 + b_3 $\n",
    "6. $ a_3 = \\sigma(z_3) $\n",
    "\n",
    "#### Key Terminology:\n",
    "- **Forward Propagation**: Passing inputs through the network to compute predictions.\n",
    "- **Hidden Layers**: Layers between input and output that apply weights, biases, and activation functions to compute outputs.\n",
    "- **Activation Function**: Function applied to layer outputs to introduce non-linearity (common functions: sigmoid, ReLU).\n",
    "- **Inference**: Using a trained model to make predictions on new data.\n",
    "\n",
    "#### Implementation Notes:\n",
    "- **TensorFlow**: In the next practical lab, you'll learn to implement this in TensorFlow, where the algorithm will be used to carry out inferences for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Code\n",
    "\n",
    "#### Key Concepts:\n",
    "- **TensorFlow**: One of the most popular deep learning frameworks, commonly used for building, training, and deploying neural networks.\n",
    "- **Inference**: The process of using a trained neural network to make predictions on new data.\n",
    "\n",
    "#### Example 1: Coffee Roasting Optimization\n",
    "- **Problem**: Given two input features—**temperature** and **duration**—predict if the roasted coffee will be good (positive label, $ y = 1 $) or bad (negative label, $ y = 0 $).\n",
    "- **Input Features**: $ x = \\left[\\text{temperature}, \\text{duration}\\right] $, e.g., $ x = [200^\\circ C, 17\\text{ minutes}] $.\n",
    "- **Neural Network Architecture**:\n",
    "  1. **Layer 1**: Dense layer with 3 neurons, sigmoid activation.\n",
    "     - Computed as $ a_1 = \\sigma(W_1 \\cdot x + b_1) $.\n",
    "     - Example output for $ a_1 $: $ a_1 = [0.2, 0.7, 0.3] $.\n",
    "  2. **Layer 2**: Dense layer with 1 neuron, sigmoid activation.\n",
    "     - Computed as $ a_2 = \\sigma(W_2 \\cdot a_1 + b_2) $.\n",
    "     - Example output for $ a_2 $: $ a_2 = 0.8 $.\n",
    "  3. **Prediction**: Threshold $ a_2 $ at 0.5:\n",
    "     - $ \\hat{y} = 1 \\text{ if } a_2 \\geq 0.5 $, otherwise $ \\hat{y} = 0 $.\n",
    "\n",
    "#### Key Steps in TensorFlow:\n",
    "- **Dense Layer**:  \n",
    "  - `Dense(units, activation='sigmoid')` creates a fully connected layer.\n",
    "- **Computing Activations**:\n",
    "  - $ a_1 = \\text{Layer1}(x) $\n",
    "  - $ a_2 = \\text{Layer2}(a_1) $\n",
    "- **Thresholding**:  \n",
    "  - $ \\hat{y} = 1 \\text{ if } a_2 \\geq 0.5 $.\n",
    "\n",
    "#### Example 2: Handwritten Digit Classification\n",
    "- **Input**: A list of pixel intensity values (e.g., 64 values for an 8x8 image).\n",
    "- **Neural Network Architecture**:\n",
    "  1. **Layer 1**: 25 neurons, sigmoid activation.\n",
    "  2. **Layer 2**: 15 neurons, sigmoid activation.\n",
    "  3. **Output Layer**: 1 neuron, sigmoid activation.\n",
    "  \n",
    "- **Steps**:\n",
    "  - Set $ x $ as a numpy array of pixel intensities.\n",
    "  - Define Layer 1:  \n",
    "    `Layer1 = Dense(25, activation='sigmoid')`\n",
    "  - Compute activations:  \n",
    "    `a1 = Layer1(x)`\n",
    "  - Similarly, define Layer 2 and Layer 3 and compute $ a2 $ and $ a3 $ using the previous activations.\n",
    "  - Threshold $ a3 $ to get the binary classification $ \\hat{y} $.\n",
    "\n",
    "#### Important TensorFlow Concepts:\n",
    "- **Numpy Arrays**: TensorFlow treats input data as numpy arrays; getting the shape and structure right is critical.\n",
    "- **Dense Layer**: A basic type of layer where each neuron is connected to every neuron in the previous layer.\n",
    "\n",
    "#### Notes:\n",
    "- **Activation Functions**: In both examples, sigmoid activation functions are used. However, depending on the problem, other activation functions like ReLU can be used.\n",
    "- **Inference with Pre-trained Models**: You can load pre-trained weights (parameters $ W $ and $ b $) into TensorFlow models and perform inference using new data.\n",
    "\n",
    "#### Additional Study Notes:\n",
    "- The labs provide hands-on practice for setting up layers and computing forward propagation in TensorFlow.\n",
    "- In forward propagation, the primary goal is to compute activations from the input to the output layer and use these values for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in TensorFlow\n",
    "\n",
    "1. **Inconsistent Data Representations**:\n",
    "   - **NumPy**: Created earlier for linear algebra, uses arrays for vectors and matrices.\n",
    "   - **TensorFlow**: Developed later for handling large datasets, uses tensors for efficiency.\n",
    "   - TensorFlow converts NumPy arrays to tensors for faster operations.\n",
    "\n",
    "2. **Matrix Representation**:\n",
    "   - A **matrix** is a 2D array of numbers.\n",
    "   - **Dimension notation**: Rows × Columns (e.g., a 2×3 matrix has 2 rows and 3 columns).\n",
    "   - Code example: `x = np.array([[1, 2, 3], [4, 5, 6]])` creates a 2×3 matrix.\n",
    "\n",
    "3. **Vector Representation**:\n",
    "   - **Row vector**: 1 row, multiple columns (e.g., a 1×2 matrix).\n",
    "   - **Column vector**: Multiple rows, 1 column (e.g., a 2×1 matrix).\n",
    "   - Example:\n",
    "     - Row vector: `np.array([[200, 17]])` (1×2 matrix).\n",
    "     - Column vector: `np.array([[200], [17]])` (2×1 matrix).\n",
    "\n",
    "4. **1D vs. 2D Arrays**:\n",
    "   - 1D arrays (used in logistic regression) have no defined rows or columns, just a list of numbers.\n",
    "   - TensorFlow prefers 2D arrays (matrices) to represent data for better computational efficiency.\n",
    "\n",
    "5. **Tensors in TensorFlow**:\n",
    "   - TensorFlow uses **tensors** (a generalized matrix) for internal operations.\n",
    "   - A tensor can be converted to a NumPy array using `.numpy()`.\n",
    "   - Example: `a1.numpy()` converts a TensorFlow tensor to a NumPy array.\n",
    "\n",
    "6. **Practical Example**:\n",
    "   - **a1**: Result of the first hidden layer; shape 1×3 (three units).\n",
    "   - **a2**: Result of the second hidden layer; shape 1×1 (single value).\n",
    "\n",
    "7. **Conversion Between TensorFlow and NumPy**:\n",
    "   - TensorFlow tensors can be converted back to NumPy arrays when needed using `.numpy()` for easier manipulation outside TensorFlow.\n",
    "   - TensorFlow automatically converts NumPy arrays into tensors during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network\n",
    "\n",
    "In this lecture, you learned about constructing neural networks in TensorFlow using a simpler approach than manually coding each layer and forward propagation step.\n",
    "\n",
    "1. **Manual Forward Propagation**: \n",
    "   - Previously, you saw how to build a neural network by manually creating layers (e.g., Layer 1 and Layer 2) and passing data through these layers step by step.\n",
    "   - Each layer’s output was manually passed to the next layer for further computation.\n",
    "\n",
    "2. **TensorFlow Sequential Model**: \n",
    "   - TensorFlow offers an easier way using the `Sequential` model, which allows you to automatically connect layers in sequence.\n",
    "   - You can create a neural network by defining the layers and telling TensorFlow to string them together, making forward propagation and learning simpler.\n",
    "\n",
    "3. **Simplified Code**:\n",
    "   - Instead of explicitly assigning layers to variables (e.g., Layer 1, Layer 2), you can directly pass the layers into the `Sequential` model for a more compact and readable code.\n",
    "   - Example:\n",
    "     ```python\n",
    "     model = Sequential([\n",
    "         Dense(3, activation='sigmoid'),  # Layer 1\n",
    "         Dense(1, activation='sigmoid')   # Layer 2\n",
    "     ])\n",
    "     ```\n",
    "\n",
    "4. **Training the Network**:\n",
    "   - To train the network, you use `model.compile()` and `model.fit()`.\n",
    "   - Example with input matrix `X` and target labels `Y`:\n",
    "     ```python\n",
    "     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "     model.fit(X, Y, epochs=10)\n",
    "     ```\n",
    "   - This simplifies the process of training the model by handling the forward and backward passes, gradient updates, etc.\n",
    "\n",
    "5. **Inference (Prediction)**:\n",
    "   - Once the model is trained, you can perform inference using `model.predict(X_new)`.\n",
    "   - This allows you to make predictions on new data easily without manually coding forward propagation for each layer.\n",
    "\n",
    "6. **Digit Classification Example**:\n",
    "   - You can apply the same approach for tasks like digit classification by sequentially defining the layers of the network and using TensorFlow’s `compile()` and `fit()` functions to train it.\n",
    "\n",
    "7. **Why Learn Manual Forward Propagation?**:\n",
    "   - Although most machine learning engineers use libraries like TensorFlow or PyTorch, it is important to understand the underlying mechanics of forward propagation.\n",
    "   - Understanding how to implement forward propagation manually will help you debug issues, adjust hyperparameters, and optimize models when necessary.\n",
    "\n",
    "### Key Takeaways:\n",
    "- TensorFlow’s `Sequential` model simplifies the process of building and training neural networks.\n",
    "- Training involves two key functions: `model.compile()` and `model.fit()`.\n",
    "- For inference, use `model.predict()`.\n",
    "- While libraries handle most tasks efficiently, understanding the inner workings of forward propagation is valuable for debugging and deeper insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward prop in a single layer\n",
    "\n",
    "To implement forward propagation from scratch in Python, the key steps involve computing the outputs of each layer in a neural network based on the input data, the weights, and biases associated with each layer, and applying an activation function (like the sigmoid function). Here’s how to break it down using a basic example:\n",
    "\n",
    "### **Step-by-Step Forward Propagation for a Single Layer**\n",
    "\n",
    "1. **Input Data**:\n",
    "   - You start with an input feature vector `x` (e.g., for a coffee roasting model), and your goal is to propagate this input through the network.\n",
    "\n",
    "2. **Weights and Biases**:\n",
    "   - Each neuron in the layer has an associated weight and bias.\n",
    "   - Example: for neuron 1 in the first layer, the weights might be `w1_1 = [1, 2]` and the bias `b1_1 = -1`.\n",
    "\n",
    "3. **Compute the Linear Combination**:\n",
    "   - The first step in forward propagation is computing the weighted sum of the inputs plus the bias (this is called the \"linear combination\" or pre-activation, denoted `z`).\n",
    "   - Example: \n",
    "     $$\n",
    "     z1_1 = w1_1 \\cdot x + b1_1\n",
    "     $$\n",
    "\n",
    "4. **Apply Activation Function**:\n",
    "   - Apply an activation function, like the sigmoid function, to the result from step 3 to get the activation `a1_1`.\n",
    "   - Sigmoid function: \n",
    "     $$\n",
    "     g(z) = \\frac{1}{1 + e^{-z}}\n",
    "     $$\n",
    "   - Example: \n",
    "     $$\n",
    "     a1_1 = g(z1_1)\n",
    "     $$\n",
    "\n",
    "5. **Repeat for All Neurons in the Layer**:\n",
    "   - Repeat the same steps for all neurons in the layer to compute all activations `a1_1`, `a1_2`, and `a1_3`.\n",
    "\n",
    "6. **Group Activations**:\n",
    "   - After computing the activations for all neurons in the first layer, group them into a vector `a1`.\n",
    "\n",
    "7. **Move to the Next Layer**:\n",
    "   - For the next layer, repeat the process: take the activations from the first layer as input, and compute the outputs for the second layer using the same steps (weighted sum, activation).\n",
    "\n",
    "### **Example Code for Forward Propagation in Python**\n",
    "\n",
    "Here’s a Python implementation using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward propagation for a single layer\n",
    "def forward_prop(x, W, b):\n",
    "    z = np.dot(W, x) + b   # Linear combination (dot product of weights and input + bias)\n",
    "    a = sigmoid(z)         # Apply activation function (sigmoid)\n",
    "    return a\n",
    "\n",
    "# Example input and parameters\n",
    "x = np.array([1, 2])      # Input vector (2 features)\n",
    "W1 = np.array([[1, 2], [-3, 4], [0.5, -1]])  # Weights for layer 1 (3 neurons, 2 inputs each)\n",
    "b1 = np.array([-1, 2, 0])                    # Bias for layer 1 (3 neurons)\n",
    "\n",
    "# Forward propagation for layer 1\n",
    "a1 = forward_prop(x, W1, b1)  # Compute activations for layer 1\n",
    "print(\"Activations for layer 1:\", a1)\n",
    "\n",
    "# Example weights and bias for layer 2\n",
    "W2 = np.array([0.5, -0.7, 1.2])  # Weights for layer 2 (1 neuron, 3 inputs)\n",
    "b2 = -0.5                         # Bias for layer 2\n",
    "\n",
    "# Forward propagation for layer 2\n",
    "a2 = forward_prop(a1, W2, b2)     # Compute activation for layer 2\n",
    "print(\"Activation for layer 2 (output):\", a2)\n",
    "```\n",
    "\n",
    "### **Explanation of the Code**:\n",
    "- **`sigmoid`**: This function applies the sigmoid activation.\n",
    "- **`forward_prop`**: This function performs forward propagation for a single layer by computing the linear combination of weights and inputs, adding the bias, and then applying the activation function.\n",
    "- **Example**: In this example, we have an input `x` with 2 features. The first layer has 3 neurons, each with its own weights and bias. We compute the activations `a1` for the first layer. Then, we use these activations as inputs for the second layer, which has 1 neuron, and compute the final output `a2`.\n",
    "\n",
    "### **Generalizing Forward Propagation for Multiple Layers**:\n",
    "To handle any number of layers, you can modify the forward propagation function to loop over all the layers in the network. Here’s how you might extend this for multiple layers:\n",
    "\n",
    "```python\n",
    "def forward_propagation(x, layers):\n",
    "    a = x  # Initial input\n",
    "    for W, b in layers:\n",
    "        a = forward_prop(a, W, b)  # Compute activations for each layer\n",
    "    return a\n",
    "\n",
    "# Example of multiple layers (layer 1 and layer 2)\n",
    "layers = [\n",
    "    (W1, b1),  # Layer 1\n",
    "    (W2, b2)   # Layer 2\n",
    "]\n",
    "\n",
    "# Forward propagation through the whole network\n",
    "output = forward_propagation(x, layers)\n",
    "print(\"Final output:\", output)\n",
    "```\n",
    "\n",
    "### **Takeaways**:\n",
    "- The key steps in forward propagation involve computing the dot product of weights and inputs, adding the bias, and applying an activation function.\n",
    "- The code above allows you to perform forward propagation manually for any number of layers in a network.\n",
    "- While libraries like TensorFlow and PyTorch automate this process, understanding how to implement it from scratch helps you gain deeper intuition about neural networks.\n",
    "\n",
    "This process is the core of what happens in frameworks like TensorFlow and PyTorch, but at a much larger scale with optimizations and flexibility for different architectures. Understanding these mechanics gives you the power to debug and even develop new features beyond what current libraries provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General implementation of forward propagation\n",
    "\n",
    "To generalize forward propagation for a neural network with multiple layers, you can use a more flexible approach that doesn't require hardcoding each neuron. Instead, you can create a function to handle dense (fully connected) layers and chain these functions to propagate inputs through the network.\n",
    "\n",
    "Here’s how you can implement a generalized forward propagation function using Python and NumPy:\n",
    "\n",
    "#### **1. Define the Dense Layer Function**\n",
    "\n",
    "The `dense` function will handle the operations for a single layer of the network. It takes the activation from the previous layer, the weights, and the biases, and computes the activations for the current layer.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Dense layer function\n",
    "def dense(a_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implements a single dense (fully connected) layer.\n",
    "    \n",
    "    Parameters:\n",
    "    a_prev -- activations from the previous layer (or input features)\n",
    "    W -- weights for the current layer\n",
    "    b -- biases for the current layer\n",
    "    \n",
    "    Returns:\n",
    "    a -- activations for the current layer\n",
    "    \"\"\"\n",
    "    # Compute the linear combination\n",
    "    z = np.dot(W.T, a_prev) + b\n",
    "    # Apply the activation function\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **`a_prev`**: The activation vector from the previous layer (or the input features if it's the first layer).\n",
    "- **`W`**: Weights matrix for the current layer.\n",
    "- **`b`**: Biases vector for the current layer.\n",
    "- **`np.dot(W.T, a_prev) + b`**: Computes the linear combination \\( z \\).\n",
    "- **`sigmoid(z)`**: Applies the sigmoid activation function to get the activations \\( a \\).\n",
    "\n",
    "#### **2. Implement Forward Propagation for Multiple Layers**\n",
    "\n",
    "You can use the `dense` function to process each layer sequentially and obtain the final output.\n",
    "\n",
    "```python\n",
    "def forward_propagation(x, parameters):\n",
    "    \"\"\"\n",
    "    Implements forward propagation through the entire network.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input features\n",
    "    parameters -- list of tuples containing (W, b) for each layer\n",
    "    \n",
    "    Returns:\n",
    "    a -- final activations (output of the network)\n",
    "    \"\"\"\n",
    "    a = x  # Start with the input features\n",
    "    for W, b in parameters:\n",
    "        a = dense(a, W, b)  # Compute activations for each layer\n",
    "    return a\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **`parameters`**: A list where each element is a tuple containing the weights and biases for each layer.\n",
    "- **`x`**: Input features.\n",
    "- **`dense(a, W, b)`**: Computes the activations for each layer sequentially.\n",
    "\n",
    "#### **3. Example Usage**\n",
    "\n",
    "Let’s assume you have a network with three layers:\n",
    "\n",
    "```python\n",
    "# Example input features\n",
    "x = np.array([0.5, 0.1])  # 2 features\n",
    "\n",
    "# Parameters for layer 1\n",
    "W1 = np.array([[1, -1], [2, 1]])  # 2x3 weights matrix\n",
    "b1 = np.array([0.5, -0.5, 0.2])    # Biases for 3 neurons\n",
    "\n",
    "# Parameters for layer 2\n",
    "W2 = np.array([[0.5, -0.3, 0.8], [0.1, 0.4, -0.6]])  # 3x2 weights matrix\n",
    "b2 = np.array([-0.1, 0.3])    # Biases for 2 neurons\n",
    "\n",
    "# Parameters for output layer\n",
    "W3 = np.array([[0.2, -0.4], [-0.5, 0.1]])  # 2x1 weights matrix\n",
    "b3 = np.array([0.2])  # Bias for 1 neuron\n",
    "\n",
    "# List of parameters for all layers\n",
    "parameters = [(W1, b1), (W2, b2), (W3, b3)]\n",
    "\n",
    "# Forward propagation\n",
    "output = forward_propagation(x, parameters)\n",
    "print(\"Final output:\", output)\n",
    "```\n",
    "\n",
    "### **Summary**\n",
    "- **`dense` Function**: Handles computations for a single layer, including linear combination and activation.\n",
    "- **`forward_propagation` Function**: Chains multiple dense layers to compute the final output of the network.\n",
    "- **Flexibility**: This approach allows you to easily adjust the number of layers and neurons without hardcoding each computation.\n",
    "\n",
    "This generalized approach helps you understand the inner workings of neural network frameworks like TensorFlow and PyTorch. By knowing how to manually implement forward propagation, you gain insights into the fundamental processes behind these powerful libraries and can more effectively troubleshoot and optimize your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculations on artificial general intelligence (AGI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a path to AGI?\n",
    "\n",
    "### **Understanding AGI and Its Challenges**\n",
    "\n",
    "Artificial General Intelligence (AGI) represents the aspiration to build AI systems with human-like intelligence. Here’s a detailed look at the concept, the current state of AI, and the potential paths toward achieving AGI:\n",
    "\n",
    "#### **1. Distinguishing ANI and AGI**\n",
    "\n",
    "- **Artificial Narrow Intelligence (ANI)**: AI systems designed for specific tasks. Examples include:\n",
    "  - **Smart Speakers**: Assist with voice commands and provide information.\n",
    "  - **Self-Driving Cars**: Navigate and make driving decisions.\n",
    "  - **Web Search Engines**: Index and retrieve information from the internet.\n",
    "\n",
    "  ANI has made significant advancements and is widely implemented in various domains, showcasing tremendous progress in AI's ability to perform specific tasks efficiently.\n",
    "\n",
    "- **Artificial General Intelligence (AGI)**: The hypothetical AI capable of understanding, learning, and applying knowledge in a way that is indistinguishable from human intelligence. AGI would:\n",
    "  - Perform any intellectual task that a human can.\n",
    "  - Exhibit general cognitive abilities and adaptability.\n",
    "\n",
    "#### **2. Current Limitations and Misconceptions**\n",
    "\n",
    "- **Over-simplification of Neural Networks**: Modern neural networks, though inspired by biological neurons, are much simpler. They use basic functions like logistic regression, which differ significantly from the complex processes occurring in human brains.\n",
    "\n",
    "- **Understanding the Brain**: Our understanding of the brain’s workings remains incomplete. Neuronal mechanisms and how they translate inputs into complex outputs are still largely unknown. Simulating the human brain with current technology is a distant goal.\n",
    "\n",
    "- **Simulating Human Intelligence**: Simply scaling up neural networks or increasing computational power might not directly lead to AGI. The complexity of human cognition involves more than just the number of simulated neurons.\n",
    "\n",
    "#### **3. Potential Paths to AGI**\n",
    "\n",
    "- **One Learning Algorithm Hypothesis**: Some experiments suggest that the brain might use a limited set of learning algorithms that can adapt to various types of input. This idea proposes that if we can discover and implement such algorithms in computers, we might approach AGI.\n",
    "\n",
    "- **Neuroscientific Experiments**:\n",
    "  - **Cross-Modal Adaptation**: Studies show that brain regions can adapt to new types of inputs, such as converting visual information to auditory processing. These experiments suggest a high level of brain plasticity and adaptability.\n",
    "  - **Sensory Substitution Devices**: Devices that translate visual or auditory information into different sensory modalities (e.g., using a tongue-mounted device to “see”) demonstrate the brain’s flexibility in interpreting various types of input.\n",
    "\n",
    "- **Future Research**: Continued research into neural network architectures, brain simulations, and learning algorithms might uncover insights necessary for AGI development. However, breakthroughs could be incremental and may take considerable time.\n",
    "\n",
    "#### **4. Realistic Expectations and Contributions**\n",
    "\n",
    "- **Avoiding Over-Hype**: AGI is a long-term goal, and while current advancements in ANI are impressive, they do not directly translate into AGI progress. The road to AGI involves substantial scientific and engineering challenges.\n",
    "\n",
    "- **Practical Applications**: Even without achieving AGI, advancements in machine learning and neural networks provide powerful tools for a wide range of applications. These technologies continue to offer significant value in areas like healthcare, finance, and more.\n",
    "\n",
    "- **Future Possibilities**: For those interested in AI research, understanding the limitations and possibilities of AGI can guide future efforts. Contributing to this field could involve exploring novel algorithms, improving neural network efficiencies, or investigating brain-computer interfaces.\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "The dream of AGI is an inspiring but complex challenge. While current progress in ANI is significant, the path to AGI involves understanding and replicating human-like intelligence, which remains a significant scientific hurdle. Continued research and practical applications of AI can provide valuable insights and advancements, even if AGI itself remains a distant goal.\n",
    "\n",
    "In the coming optional videos, exploring efficient neural network implementations and vectorization techniques will further enhance your understanding and capabilities in AI, regardless of the AGI aspiration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
